/**
 * Generated by orval v7.7.0 üç∫
 * Do not edit manually.
 * OpenAI API
 * The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
 * OpenAPI spec version: 2.3.0
 */
import {
  z as zod
} from 'zod';


/**
 * Create a new Realtime API call over WebRTC and receive the SDP answer needed
to complete the peer connection.
 * @summary Create call
 */
export const createRealtimeCallBodySessionOutputModalitiesDefault = ["audio"];export const createRealtimeCallBodySessionAudioInputTurnDetectionTypeDefault = "server_vad";export const createRealtimeCallBodySessionAudioInputTurnDetectionCreateResponseDefault = true;export const createRealtimeCallBodySessionAudioInputTurnDetectionInterruptResponseDefault = true;export const createRealtimeCallBodySessionAudioInputTurnDetectionIdleTimeoutMsMinOne = 5000;
export const createRealtimeCallBodySessionAudioInputTurnDetectionIdleTimeoutMsMaxOne = 30000;
export const createRealtimeCallBodySessionAudioInputTurnDetectionEagernessDefault = "auto";export const createRealtimeCallBodySessionAudioInputTurnDetectionCreateResponseDefaultOne = true;export const createRealtimeCallBodySessionAudioInputTurnDetectionInterruptResponseDefaultOne = true;export const createRealtimeCallBodySessionAudioOutputSpeedDefault = 1;
export const createRealtimeCallBodySessionAudioOutputSpeedMin = 0.25;

export const createRealtimeCallBodySessionAudioOutputSpeedMax = 1.5;
export const createRealtimeCallBodySessionTracingDefaultOne = "auto";export const createRealtimeCallBodySessionToolsItemRequireApprovalDefaultOne = "always";export const createRealtimeCallBodySessionToolChoiceDefault = "auto";export const createRealtimeCallBodySessionTruncationRetentionRatioMin = 0;

export const createRealtimeCallBodySessionTruncationRetentionRatioMax = 1;
export const createRealtimeCallBodySessionPromptVariablesTypeDefault = "input_text";export const createRealtimeCallBodySessionPromptVariablesTypeDefaultOne = "input_image";export const createRealtimeCallBodySessionPromptVariablesTypeDefaultTwo = "input_file";

export const createRealtimeCallBody = zod.object({
  "sdp": zod.string().describe('WebRTC Session Description Protocol (SDP) offer generated by the caller.'),
  "session": zod.object({
  "type": zod.enum(['realtime']).describe('The type of session to create. Always `realtime` for the Realtime API.\n'),
  "output_modalities": zod.array(zod.enum(['text', 'audio'])).optional().describe('The set of modalities the model can respond with. It defaults to `[\"audio\"]`, indicating\nthat the model will respond with audio plus a transcript. `[\"text\"]` can be used to make\nthe model respond with text only. It is not possible to request both `text` and `audio` at the same time.\n'),
  "model": zod.string().or(zod.enum(['gpt-realtime', 'gpt-realtime-2025-08-28', 'gpt-4o-realtime-preview', 'gpt-4o-realtime-preview-2024-10-01', 'gpt-4o-realtime-preview-2024-12-17', 'gpt-4o-realtime-preview-2025-06-03', 'gpt-4o-mini-realtime-preview', 'gpt-4o-mini-realtime-preview-2024-12-17', 'gpt-realtime-mini', 'gpt-realtime-mini-2025-10-06', 'gpt-audio-mini', 'gpt-audio-mini-2025-10-06'])).optional().describe('The Realtime model used for this session.\n'),
  "instructions": zod.string().optional().describe('The default system instructions (i.e. system message) prepended to model calls. This field allows the client to guide the model on desired responses. The model can be instructed on response content and format, (e.g. \"be extremely succinct\", \"act friendly\", \"here are examples of good responses\") and on audio behavior (e.g. \"talk quickly\", \"inject emotion into your voice\", \"laugh frequently\"). The instructions are not guaranteed to be followed by the model, but they provide guidance to the model on the desired behavior.\n\nNote that the server sets default instructions which will be used if this field is not set and are visible in the `session.created` event at the start of the session.\n'),
  "audio": zod.object({
  "input": zod.object({
  "format": zod.discriminatedUnion('type', [zod.object({
  "type": zod.enum(['audio/pcm']).optional().describe('The audio format. Always `audio/pcm`.'),
  "rate": zod.number().optional().describe('The sample rate of the audio. Always `24000`.')
}).describe('The PCM audio format. Only a 24kHz sample rate is supported.'),zod.object({
  "type": zod.enum(['audio/pcmu']).optional().describe('The audio format. Always `audio/pcmu`.')
}).describe('The G.711 Œº-law format.'),zod.object({
  "type": zod.enum(['audio/pcma']).optional().describe('The audio format. Always `audio/pcma`.')
}).describe('The G.711 A-law format.')]).optional(),
  "transcription": zod.object({
  "model": zod.enum(['whisper-1', 'gpt-4o-mini-transcribe', 'gpt-4o-transcribe', 'gpt-4o-transcribe-diarize']).optional().describe('The model to use for transcription. Current options are `whisper-1`, `gpt-4o-mini-transcribe`, `gpt-4o-transcribe`, and `gpt-4o-transcribe-diarize`. Use `gpt-4o-transcribe-diarize` when you need diarization with speaker labels.\n'),
  "language": zod.string().optional().describe('The language of the input audio. Supplying the input language in\n[ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`) format\nwill improve accuracy and latency.\n'),
  "prompt": zod.string().optional().describe('An optional text to guide the model\'s style or continue a previous audio\nsegment.\nFor `whisper-1`, the [prompt is a list of keywords](https://platform.openai.com/docs/guides/speech-to-text#prompting).\nFor `gpt-4o-transcribe` models (excluding `gpt-4o-transcribe-diarize`), the prompt is a free text string, for example \"expect words related to technology\".\n')
}).optional(),
  "noise_reduction": zod.object({
  "type": zod.enum(['near_field', 'far_field']).optional().describe('Type of noise reduction. `near_field` is for close-talking microphones such as headphones, `far_field` is for far-field microphones such as laptop or conference room microphones.\n')
}).optional().describe('Configuration for input audio noise reduction. This can be set to `null` to turn off.\nNoise reduction filters audio added to the input audio buffer before it is sent to VAD and the model.\nFiltering the audio can improve VAD and turn detection accuracy (reducing false positives) and model performance by improving perception of the input audio.\n'),
  "turn_detection": zod.discriminatedUnion('type', [zod.object({
  "type": zod.string().optional().describe('Type of turn detection, `server_vad` to turn on simple Server VAD.\n'),
  "threshold": zod.number().optional().describe('Used only for `server_vad` mode. Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A\nhigher threshold will require louder audio to activate the model, and\nthus might perform better in noisy environments.\n'),
  "prefix_padding_ms": zod.number().optional().describe('Used only for `server_vad` mode. Amount of audio to include before the VAD detected speech (in\nmilliseconds). Defaults to 300ms.\n'),
  "silence_duration_ms": zod.number().optional().describe('Used only for `server_vad` mode. Duration of silence to detect speech stop (in milliseconds). Defaults\nto 500ms. With shorter values the model will respond more quickly,\nbut may jump in on short pauses from the user.\n'),
  "create_response": zod.boolean().optional().describe('Whether or not to automatically generate a response when a VAD stop event occurs.\n'),
  "interrupt_response": zod.boolean().optional().describe('Whether or not to automatically interrupt any ongoing response with output to the default\nconversation (i.e. `conversation` of `auto`) when a VAD start event occurs.\n'),
  "idle_timeout_ms": zod.number().min(createRealtimeCallBodySessionAudioInputTurnDetectionIdleTimeoutMsMinOne).max(createRealtimeCallBodySessionAudioInputTurnDetectionIdleTimeoutMsMaxOne).describe('Optional timeout after which a model response will be triggered automatically. This is\nuseful for situations in which a long pause from the user is unexpected, such as a phone\ncall. The model will effectively prompt the user to continue the conversation based\non the current context.\n\nThe timeout value will be applied after the last model response\'s audio has finished playing,\ni.e. it\'s set to the `response.done` time plus audio playback duration.\n\nAn `input_audio_buffer.timeout_triggered` event (plus events\nassociated with the Response) will be emitted when the timeout is reached.\nIdle timeout is currently only supported for `server_vad` mode.\n').or(zod.null()).optional()
}).describe('Server-side voice activity detection (VAD) which flips on when user speech is detected and off after a period of silence.'),zod.object({
  "type": zod.string().describe('Type of turn detection, `semantic_vad` to turn on Semantic VAD.\n'),
  "eagerness": zod.enum(['low', 'medium', 'high', 'auto']).optional().describe('Used only for `semantic_vad` mode. The eagerness of the model to respond. `low` will wait longer for the user to continue speaking, `high` will respond more quickly. `auto` is the default and is equivalent to `medium`. `low`, `medium`, and `high` have max timeouts of 8s, 4s, and 2s respectively.\n'),
  "create_response": zod.boolean().optional().describe('Whether or not to automatically generate a response when a VAD stop event occurs.\n'),
  "interrupt_response": zod.boolean().optional().describe('Whether or not to automatically interrupt any ongoing response with output to the default\nconversation (i.e. `conversation` of `auto`) when a VAD start event occurs.\n')
}).describe('Server-side semantic turn detection which uses a model to determine when the user has finished speaking.')]).describe('Configuration for turn detection, ether Server VAD or Semantic VAD. This can be set to `null` to turn off, in which case the client must manually trigger model response.\n\nServer VAD means that the model will detect the start and end of speech based on audio volume and respond at the end of user speech.\n\nSemantic VAD is more advanced and uses a turn detection model (in conjunction with VAD) to semantically estimate whether the user has finished speaking, then dynamically sets a timeout based on this probability. For example, if user audio trails off with \"uhhm\", the model will score a low probability of turn end and wait longer for the user to continue speaking. This can be useful for more natural conversations, but may have a higher latency.\n').or(zod.null()).optional()
}).optional(),
  "output": zod.object({
  "format": zod.discriminatedUnion('type', [zod.object({
  "type": zod.enum(['audio/pcm']).optional().describe('The audio format. Always `audio/pcm`.'),
  "rate": zod.number().optional().describe('The sample rate of the audio. Always `24000`.')
}).describe('The PCM audio format. Only a 24kHz sample rate is supported.'),zod.object({
  "type": zod.enum(['audio/pcmu']).optional().describe('The audio format. Always `audio/pcmu`.')
}).describe('The G.711 Œº-law format.'),zod.object({
  "type": zod.enum(['audio/pcma']).optional().describe('The audio format. Always `audio/pcma`.')
}).describe('The G.711 A-law format.')]).optional(),
  "voice": zod.string().or(zod.enum(['alloy', 'ash', 'ballad', 'coral', 'echo', 'sage', 'shimmer', 'verse', 'marin', 'cedar'])).optional(),
  "speed": zod.number().min(createRealtimeCallBodySessionAudioOutputSpeedMin).max(createRealtimeCallBodySessionAudioOutputSpeedMax).optional().describe('The speed of the model\'s spoken response as a multiple of the original speed.\n1.0 is the default speed. 0.25 is the minimum speed. 1.5 is the maximum speed. This value can only be changed in between model turns, not while a response is in progress.\n\nThis parameter is a post-processing adjustment to the audio after it is generated, it\'s\nalso possible to prompt the model to speak faster or slower.\n')
}).optional()
}).optional().describe('Configuration for input and output audio.\n'),
  "include": zod.array(zod.enum(['item.input_audio_transcription.logprobs'])).optional().describe('Additional fields to include in server outputs.\n\n`item.input_audio_transcription.logprobs`: Include logprobs for input audio transcription.\n'),
  "tracing": zod.enum(['auto']).optional().describe('Default tracing mode for the session.\n').or(zod.object({
  "workflow_name": zod.string().optional().describe('The name of the workflow to attach to this trace. This is used to\nname the trace in the Traces Dashboard.\n'),
  "group_id": zod.string().optional().describe('The group id to attach to this trace to enable filtering and\ngrouping in the Traces Dashboard.\n'),
  "metadata": zod.object({

}).optional().describe('The arbitrary metadata to attach to this trace to enable\nfiltering in the Traces Dashboard.\n')
}).describe('Granular configuration for tracing.\n')).nullish().describe('Realtime API can write session traces to the [Traces Dashboard](/logs?api=traces). Set to null to disable tracing. Once\ntracing is enabled for a session, the configuration cannot be modified.\n\n`auto` will create a trace for the session with default values for the\nworkflow name, group id, and metadata.\n'),
  "tools": zod.array(zod.discriminatedUnion('type', [zod.object({
  "type": zod.enum(['function']).optional().describe('The type of the tool, i.e. `function`.'),
  "name": zod.string().optional().describe('The name of the function.'),
  "description": zod.string().optional().describe('The description of the function, including guidance on when and how\nto call it, and guidance about what to tell the user when calling\n(if anything).\n'),
  "parameters": zod.object({

}).optional().describe('Parameters of the function in JSON Schema.')
}),zod.object({
  "type": zod.enum(['mcp']).describe('The type of the MCP tool. Always `mcp`.'),
  "server_label": zod.string().describe('A label for this MCP server, used to identify it in tool calls.\n'),
  "server_url": zod.string().optional().describe('The URL for the MCP server. One of `server_url` or `connector_id` must be\nprovided.\n'),
  "connector_id": zod.enum(['connector_dropbox', 'connector_gmail', 'connector_googlecalendar', 'connector_googledrive', 'connector_microsoftteams', 'connector_outlookcalendar', 'connector_outlookemail', 'connector_sharepoint']).optional().describe('Identifier for service connectors, like those available in ChatGPT. One of\n`server_url` or `connector_id` must be provided. Learn more about service\nconnectors [here](https://platform.openai.com/docs/guides/tools-remote-mcp#connectors).\n\nCurrently supported `connector_id` values are:\n\n- Dropbox: `connector_dropbox`\n- Gmail: `connector_gmail`\n- Google Calendar: `connector_googlecalendar`\n- Google Drive: `connector_googledrive`\n- Microsoft Teams: `connector_microsoftteams`\n- Outlook Calendar: `connector_outlookcalendar`\n- Outlook Email: `connector_outlookemail`\n- SharePoint: `connector_sharepoint`\n'),
  "authorization": zod.string().optional().describe('An OAuth access token that can be used with a remote MCP server, either\nwith a custom MCP server URL or a service connector. Your application\nmust handle the OAuth authorization flow and provide the token here.\n'),
  "server_description": zod.string().optional().describe('Optional description of the MCP server, used to provide more context.\n'),
  "headers": zod.record(zod.string(), zod.string()).describe('Optional HTTP headers to send to the MCP server. Use for authentication\nor other purposes.\n').or(zod.null()).optional(),
  "allowed_tools": zod.array(zod.string()).describe('A string array of allowed tool names').or(zod.object({
  "tool_names": zod.array(zod.string()).optional().describe('List of allowed tool names.'),
  "read_only": zod.boolean().optional().describe('Indicates whether or not a tool modifies data or is read-only. If an\nMCP server is [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\nit will match this filter.\n')
}).describe('A filter object to specify which tools are allowed.\n')).describe('List of allowed tool names or a filter object.\n').or(zod.null()).optional(),
  "require_approval": zod.object({
  "always": zod.object({
  "tool_names": zod.array(zod.string()).optional().describe('List of allowed tool names.'),
  "read_only": zod.boolean().optional().describe('Indicates whether or not a tool modifies data or is read-only. If an\nMCP server is [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\nit will match this filter.\n')
}).optional().describe('A filter object to specify which tools are allowed.\n'),
  "never": zod.object({
  "tool_names": zod.array(zod.string()).optional().describe('List of allowed tool names.'),
  "read_only": zod.boolean().optional().describe('Indicates whether or not a tool modifies data or is read-only. If an\nMCP server is [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\nit will match this filter.\n')
}).optional().describe('A filter object to specify which tools are allowed.\n')
}).describe('Specify which of the MCP server\'s tools require approval. Can be\n`always`, `never`, or a filter object associated with tools\nthat require approval.\n').or(zod.enum(['always', 'never']).describe('Specify a single approval policy for all tools. One of `always` or\n`never`. When set to `always`, all tools will require approval. When\nset to `never`, all tools will not require approval.\n')).optional().describe('Specify which of the MCP server\'s tools require approval.').or(zod.null()).optional()
}).describe('Give the model access to additional tools via remote Model Context Protocol\n(MCP) servers. [Learn more about MCP](https://platform.openai.com/docs/guides/tools-remote-mcp).\n')])).optional().describe('Tools available to the model.'),
  "tool_choice": zod.enum(['none', 'auto', 'required']).describe('Controls which (if any) tool is called by the model.\n\n`none` means the model will not call any tool and instead generates a message.\n\n`auto` means the model can pick between generating a message or calling one or\nmore tools.\n\n`required` means the model must call one or more tools.\n').or(zod.object({
  "type": zod.enum(['function']).describe('For function calling, the type is always `function`.'),
  "name": zod.string().describe('The name of the function to call.')
}).describe('Use this option to force the model to call a specific function.\n')).or(zod.object({
  "type": zod.enum(['mcp']).describe('For MCP tools, the type is always `mcp`.'),
  "server_label": zod.string().describe('The label of the MCP server to use.\n'),
  "name": zod.string().describe('The name of the tool to call on the server.\n').or(zod.null()).optional()
}).describe('Use this option to force the model to call a specific tool on a remote MCP server.\n')).optional().describe('How the model chooses tools. Provide one of the string modes or force a specific\nfunction/MCP tool.\n'),
  "max_output_tokens": zod.number().or(zod.enum(['inf'])).optional().describe('Maximum number of output tokens for a single assistant response,\ninclusive of tool calls. Provide an integer between 1 and 4096 to\nlimit output tokens, or `inf` for the maximum available tokens for a\ngiven model. Defaults to `inf`.\n'),
  "truncation": zod.enum(['auto', 'disabled']).describe('The truncation strategy to use for the session. `auto` is the default truncation strategy. `disabled` will disable truncation and emit errors when the conversation exceeds the input token limit.').or(zod.object({
  "type": zod.enum(['retention_ratio']).describe('Use retention ratio truncation.'),
  "retention_ratio": zod.number().min(createRealtimeCallBodySessionTruncationRetentionRatioMin).max(createRealtimeCallBodySessionTruncationRetentionRatioMax).describe('Fraction of post-instruction conversation tokens to retain (0.0 - 1.0) when the conversation exceeds the input token limit.\n')
}).describe('Retain a fraction of the conversation tokens when the conversation exceeds the input token limit. This allows you to amortize truncations across multiple turns, which can help improve cached token usage.')).optional().describe('Controls how the realtime conversation is truncated prior to model inference.\nThe default is `auto`.\n'),
  "prompt": zod.object({
  "id": zod.string().describe('The unique identifier of the prompt template to use.'),
  "version": zod.string().describe('Optional version of the prompt template.').or(zod.null()).optional(),
  "variables": zod.record(zod.string(), zod.string().or(zod.object({
  "type": zod.enum(['input_text']).optional().describe('The type of the input item. Always `input_text`.'),
  "text": zod.string().describe('The text input to the model.')
}).describe('A text input to the model.')).or(zod.object({
  "type": zod.enum(['input_image']).optional().describe('The type of the input item. Always `input_image`.'),
  "image_url": zod.string().describe('The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL.').or(zod.null()).optional(),
  "file_id": zod.string().describe('The ID of the file to be sent to the model.').or(zod.null()).optional(),
  "detail": zod.enum(['low', 'high', 'auto'])
}).describe('An image input to the model. Learn about [image inputs](https://platform.openai.com/docs/guides/vision).')).or(zod.object({
  "type": zod.enum(['input_file']).optional().describe('The type of the input item. Always `input_file`.'),
  "file_id": zod.string().describe('The ID of the file to be sent to the model.').or(zod.null()).optional(),
  "filename": zod.string().optional().describe('The name of the file to be sent to the model.'),
  "file_url": zod.string().optional().describe('The URL of the file to be sent to the model.'),
  "file_data": zod.string().optional().describe('The content of the file to be sent to the model.\n')
}).describe('A file input to the model.'))).describe('Optional map of values to substitute in for variables in your\nprompt. The substitution values can either be strings, or other\nResponse input types like images or files.\n').or(zod.null()).optional()
}).describe('Reference to a prompt template and its variables.\n[Learn more](https://platform.openai.com/docs/guides/text?api-mode=responses#reusable-prompts).\n').or(zod.null()).optional()
}).describe('Realtime session object configuration.').optional().describe('Optional session configuration to apply before the realtime session is\ncreated. Use the same parameters you would send in a [`create client secret`](https://platform.openai.com/docs/api-reference/realtime-sessions/create-realtime-client-secret)\nrequest.')
}).describe('Parameters required to initiate a realtime call and receive the SDP answer\nneeded to complete a WebRTC peer connection. Provide an SDP offer generated\nby your client and optionally configure the session that will answer the call.')

/**
 * Accept an incoming SIP call and configure the realtime session that will
handle it.
 * @summary Accept call
 */
export const acceptRealtimeCallParams = zod.object({
  "call_id": zod.string().describe('The identifier for the call provided in the\n[`realtime.call.incoming`](https://platform.openai.com/docs/api-reference/webhook_events/realtime/call/incoming)\nwebhook.')
})

export const acceptRealtimeCallBodyOutputModalitiesDefault = ["audio"];export const acceptRealtimeCallBodyAudioInputTurnDetectionTypeDefault = "server_vad";export const acceptRealtimeCallBodyAudioInputTurnDetectionCreateResponseDefault = true;export const acceptRealtimeCallBodyAudioInputTurnDetectionInterruptResponseDefault = true;export const acceptRealtimeCallBodyAudioInputTurnDetectionIdleTimeoutMsMinOne = 5000;
export const acceptRealtimeCallBodyAudioInputTurnDetectionIdleTimeoutMsMaxOne = 30000;
export const acceptRealtimeCallBodyAudioInputTurnDetectionEagernessDefault = "auto";export const acceptRealtimeCallBodyAudioInputTurnDetectionCreateResponseDefaultOne = true;export const acceptRealtimeCallBodyAudioInputTurnDetectionInterruptResponseDefaultOne = true;export const acceptRealtimeCallBodyAudioOutputSpeedDefault = 1;
export const acceptRealtimeCallBodyAudioOutputSpeedMin = 0.25;

export const acceptRealtimeCallBodyAudioOutputSpeedMax = 1.5;
export const acceptRealtimeCallBodyTracingDefaultOne = "auto";export const acceptRealtimeCallBodyToolsItemRequireApprovalDefaultOne = "always";export const acceptRealtimeCallBodyToolChoiceDefault = "auto";export const acceptRealtimeCallBodyTruncationRetentionRatioMin = 0;

export const acceptRealtimeCallBodyTruncationRetentionRatioMax = 1;
export const acceptRealtimeCallBodyPromptVariablesTypeDefault = "input_text";export const acceptRealtimeCallBodyPromptVariablesTypeDefaultOne = "input_image";export const acceptRealtimeCallBodyPromptVariablesTypeDefaultTwo = "input_file";

export const acceptRealtimeCallBody = zod.object({
  "type": zod.enum(['realtime']).describe('The type of session to create. Always `realtime` for the Realtime API.\n'),
  "output_modalities": zod.array(zod.enum(['text', 'audio'])).optional().describe('The set of modalities the model can respond with. It defaults to `[\"audio\"]`, indicating\nthat the model will respond with audio plus a transcript. `[\"text\"]` can be used to make\nthe model respond with text only. It is not possible to request both `text` and `audio` at the same time.\n'),
  "model": zod.string().or(zod.enum(['gpt-realtime', 'gpt-realtime-2025-08-28', 'gpt-4o-realtime-preview', 'gpt-4o-realtime-preview-2024-10-01', 'gpt-4o-realtime-preview-2024-12-17', 'gpt-4o-realtime-preview-2025-06-03', 'gpt-4o-mini-realtime-preview', 'gpt-4o-mini-realtime-preview-2024-12-17', 'gpt-realtime-mini', 'gpt-realtime-mini-2025-10-06', 'gpt-audio-mini', 'gpt-audio-mini-2025-10-06'])).optional().describe('The Realtime model used for this session.\n'),
  "instructions": zod.string().optional().describe('The default system instructions (i.e. system message) prepended to model calls. This field allows the client to guide the model on desired responses. The model can be instructed on response content and format, (e.g. \"be extremely succinct\", \"act friendly\", \"here are examples of good responses\") and on audio behavior (e.g. \"talk quickly\", \"inject emotion into your voice\", \"laugh frequently\"). The instructions are not guaranteed to be followed by the model, but they provide guidance to the model on the desired behavior.\n\nNote that the server sets default instructions which will be used if this field is not set and are visible in the `session.created` event at the start of the session.\n'),
  "audio": zod.object({
  "input": zod.object({
  "format": zod.discriminatedUnion('type', [zod.object({
  "type": zod.enum(['audio/pcm']).optional().describe('The audio format. Always `audio/pcm`.'),
  "rate": zod.number().optional().describe('The sample rate of the audio. Always `24000`.')
}).describe('The PCM audio format. Only a 24kHz sample rate is supported.'),zod.object({
  "type": zod.enum(['audio/pcmu']).optional().describe('The audio format. Always `audio/pcmu`.')
}).describe('The G.711 Œº-law format.'),zod.object({
  "type": zod.enum(['audio/pcma']).optional().describe('The audio format. Always `audio/pcma`.')
}).describe('The G.711 A-law format.')]).optional(),
  "transcription": zod.object({
  "model": zod.enum(['whisper-1', 'gpt-4o-mini-transcribe', 'gpt-4o-transcribe', 'gpt-4o-transcribe-diarize']).optional().describe('The model to use for transcription. Current options are `whisper-1`, `gpt-4o-mini-transcribe`, `gpt-4o-transcribe`, and `gpt-4o-transcribe-diarize`. Use `gpt-4o-transcribe-diarize` when you need diarization with speaker labels.\n'),
  "language": zod.string().optional().describe('The language of the input audio. Supplying the input language in\n[ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`) format\nwill improve accuracy and latency.\n'),
  "prompt": zod.string().optional().describe('An optional text to guide the model\'s style or continue a previous audio\nsegment.\nFor `whisper-1`, the [prompt is a list of keywords](https://platform.openai.com/docs/guides/speech-to-text#prompting).\nFor `gpt-4o-transcribe` models (excluding `gpt-4o-transcribe-diarize`), the prompt is a free text string, for example \"expect words related to technology\".\n')
}).optional(),
  "noise_reduction": zod.object({
  "type": zod.enum(['near_field', 'far_field']).optional().describe('Type of noise reduction. `near_field` is for close-talking microphones such as headphones, `far_field` is for far-field microphones such as laptop or conference room microphones.\n')
}).optional().describe('Configuration for input audio noise reduction. This can be set to `null` to turn off.\nNoise reduction filters audio added to the input audio buffer before it is sent to VAD and the model.\nFiltering the audio can improve VAD and turn detection accuracy (reducing false positives) and model performance by improving perception of the input audio.\n'),
  "turn_detection": zod.discriminatedUnion('type', [zod.object({
  "type": zod.string().optional().describe('Type of turn detection, `server_vad` to turn on simple Server VAD.\n'),
  "threshold": zod.number().optional().describe('Used only for `server_vad` mode. Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A\nhigher threshold will require louder audio to activate the model, and\nthus might perform better in noisy environments.\n'),
  "prefix_padding_ms": zod.number().optional().describe('Used only for `server_vad` mode. Amount of audio to include before the VAD detected speech (in\nmilliseconds). Defaults to 300ms.\n'),
  "silence_duration_ms": zod.number().optional().describe('Used only for `server_vad` mode. Duration of silence to detect speech stop (in milliseconds). Defaults\nto 500ms. With shorter values the model will respond more quickly,\nbut may jump in on short pauses from the user.\n'),
  "create_response": zod.boolean().optional().describe('Whether or not to automatically generate a response when a VAD stop event occurs.\n'),
  "interrupt_response": zod.boolean().optional().describe('Whether or not to automatically interrupt any ongoing response with output to the default\nconversation (i.e. `conversation` of `auto`) when a VAD start event occurs.\n'),
  "idle_timeout_ms": zod.number().min(acceptRealtimeCallBodyAudioInputTurnDetectionIdleTimeoutMsMinOne).max(acceptRealtimeCallBodyAudioInputTurnDetectionIdleTimeoutMsMaxOne).describe('Optional timeout after which a model response will be triggered automatically. This is\nuseful for situations in which a long pause from the user is unexpected, such as a phone\ncall. The model will effectively prompt the user to continue the conversation based\non the current context.\n\nThe timeout value will be applied after the last model response\'s audio has finished playing,\ni.e. it\'s set to the `response.done` time plus audio playback duration.\n\nAn `input_audio_buffer.timeout_triggered` event (plus events\nassociated with the Response) will be emitted when the timeout is reached.\nIdle timeout is currently only supported for `server_vad` mode.\n').or(zod.null()).optional()
}).describe('Server-side voice activity detection (VAD) which flips on when user speech is detected and off after a period of silence.'),zod.object({
  "type": zod.string().describe('Type of turn detection, `semantic_vad` to turn on Semantic VAD.\n'),
  "eagerness": zod.enum(['low', 'medium', 'high', 'auto']).optional().describe('Used only for `semantic_vad` mode. The eagerness of the model to respond. `low` will wait longer for the user to continue speaking, `high` will respond more quickly. `auto` is the default and is equivalent to `medium`. `low`, `medium`, and `high` have max timeouts of 8s, 4s, and 2s respectively.\n'),
  "create_response": zod.boolean().optional().describe('Whether or not to automatically generate a response when a VAD stop event occurs.\n'),
  "interrupt_response": zod.boolean().optional().describe('Whether or not to automatically interrupt any ongoing response with output to the default\nconversation (i.e. `conversation` of `auto`) when a VAD start event occurs.\n')
}).describe('Server-side semantic turn detection which uses a model to determine when the user has finished speaking.')]).describe('Configuration for turn detection, ether Server VAD or Semantic VAD. This can be set to `null` to turn off, in which case the client must manually trigger model response.\n\nServer VAD means that the model will detect the start and end of speech based on audio volume and respond at the end of user speech.\n\nSemantic VAD is more advanced and uses a turn detection model (in conjunction with VAD) to semantically estimate whether the user has finished speaking, then dynamically sets a timeout based on this probability. For example, if user audio trails off with \"uhhm\", the model will score a low probability of turn end and wait longer for the user to continue speaking. This can be useful for more natural conversations, but may have a higher latency.\n').or(zod.null()).optional()
}).optional(),
  "output": zod.object({
  "format": zod.discriminatedUnion('type', [zod.object({
  "type": zod.enum(['audio/pcm']).optional().describe('The audio format. Always `audio/pcm`.'),
  "rate": zod.number().optional().describe('The sample rate of the audio. Always `24000`.')
}).describe('The PCM audio format. Only a 24kHz sample rate is supported.'),zod.object({
  "type": zod.enum(['audio/pcmu']).optional().describe('The audio format. Always `audio/pcmu`.')
}).describe('The G.711 Œº-law format.'),zod.object({
  "type": zod.enum(['audio/pcma']).optional().describe('The audio format. Always `audio/pcma`.')
}).describe('The G.711 A-law format.')]).optional(),
  "voice": zod.string().or(zod.enum(['alloy', 'ash', 'ballad', 'coral', 'echo', 'sage', 'shimmer', 'verse', 'marin', 'cedar'])).optional(),
  "speed": zod.number().min(acceptRealtimeCallBodyAudioOutputSpeedMin).max(acceptRealtimeCallBodyAudioOutputSpeedMax).optional().describe('The speed of the model\'s spoken response as a multiple of the original speed.\n1.0 is the default speed. 0.25 is the minimum speed. 1.5 is the maximum speed. This value can only be changed in between model turns, not while a response is in progress.\n\nThis parameter is a post-processing adjustment to the audio after it is generated, it\'s\nalso possible to prompt the model to speak faster or slower.\n')
}).optional()
}).optional().describe('Configuration for input and output audio.\n'),
  "include": zod.array(zod.enum(['item.input_audio_transcription.logprobs'])).optional().describe('Additional fields to include in server outputs.\n\n`item.input_audio_transcription.logprobs`: Include logprobs for input audio transcription.\n'),
  "tracing": zod.enum(['auto']).optional().describe('Default tracing mode for the session.\n').or(zod.object({
  "workflow_name": zod.string().optional().describe('The name of the workflow to attach to this trace. This is used to\nname the trace in the Traces Dashboard.\n'),
  "group_id": zod.string().optional().describe('The group id to attach to this trace to enable filtering and\ngrouping in the Traces Dashboard.\n'),
  "metadata": zod.object({

}).optional().describe('The arbitrary metadata to attach to this trace to enable\nfiltering in the Traces Dashboard.\n')
}).describe('Granular configuration for tracing.\n')).nullish().describe('Realtime API can write session traces to the [Traces Dashboard](/logs?api=traces). Set to null to disable tracing. Once\ntracing is enabled for a session, the configuration cannot be modified.\n\n`auto` will create a trace for the session with default values for the\nworkflow name, group id, and metadata.\n'),
  "tools": zod.array(zod.discriminatedUnion('type', [zod.object({
  "type": zod.enum(['function']).optional().describe('The type of the tool, i.e. `function`.'),
  "name": zod.string().optional().describe('The name of the function.'),
  "description": zod.string().optional().describe('The description of the function, including guidance on when and how\nto call it, and guidance about what to tell the user when calling\n(if anything).\n'),
  "parameters": zod.object({

}).optional().describe('Parameters of the function in JSON Schema.')
}),zod.object({
  "type": zod.enum(['mcp']).describe('The type of the MCP tool. Always `mcp`.'),
  "server_label": zod.string().describe('A label for this MCP server, used to identify it in tool calls.\n'),
  "server_url": zod.string().optional().describe('The URL for the MCP server. One of `server_url` or `connector_id` must be\nprovided.\n'),
  "connector_id": zod.enum(['connector_dropbox', 'connector_gmail', 'connector_googlecalendar', 'connector_googledrive', 'connector_microsoftteams', 'connector_outlookcalendar', 'connector_outlookemail', 'connector_sharepoint']).optional().describe('Identifier for service connectors, like those available in ChatGPT. One of\n`server_url` or `connector_id` must be provided. Learn more about service\nconnectors [here](https://platform.openai.com/docs/guides/tools-remote-mcp#connectors).\n\nCurrently supported `connector_id` values are:\n\n- Dropbox: `connector_dropbox`\n- Gmail: `connector_gmail`\n- Google Calendar: `connector_googlecalendar`\n- Google Drive: `connector_googledrive`\n- Microsoft Teams: `connector_microsoftteams`\n- Outlook Calendar: `connector_outlookcalendar`\n- Outlook Email: `connector_outlookemail`\n- SharePoint: `connector_sharepoint`\n'),
  "authorization": zod.string().optional().describe('An OAuth access token that can be used with a remote MCP server, either\nwith a custom MCP server URL or a service connector. Your application\nmust handle the OAuth authorization flow and provide the token here.\n'),
  "server_description": zod.string().optional().describe('Optional description of the MCP server, used to provide more context.\n'),
  "headers": zod.record(zod.string(), zod.string()).describe('Optional HTTP headers to send to the MCP server. Use for authentication\nor other purposes.\n').or(zod.null()).optional(),
  "allowed_tools": zod.array(zod.string()).describe('A string array of allowed tool names').or(zod.object({
  "tool_names": zod.array(zod.string()).optional().describe('List of allowed tool names.'),
  "read_only": zod.boolean().optional().describe('Indicates whether or not a tool modifies data or is read-only. If an\nMCP server is [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\nit will match this filter.\n')
}).describe('A filter object to specify which tools are allowed.\n')).describe('List of allowed tool names or a filter object.\n').or(zod.null()).optional(),
  "require_approval": zod.object({
  "always": zod.object({
  "tool_names": zod.array(zod.string()).optional().describe('List of allowed tool names.'),
  "read_only": zod.boolean().optional().describe('Indicates whether or not a tool modifies data or is read-only. If an\nMCP server is [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\nit will match this filter.\n')
}).optional().describe('A filter object to specify which tools are allowed.\n'),
  "never": zod.object({
  "tool_names": zod.array(zod.string()).optional().describe('List of allowed tool names.'),
  "read_only": zod.boolean().optional().describe('Indicates whether or not a tool modifies data or is read-only. If an\nMCP server is [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\nit will match this filter.\n')
}).optional().describe('A filter object to specify which tools are allowed.\n')
}).describe('Specify which of the MCP server\'s tools require approval. Can be\n`always`, `never`, or a filter object associated with tools\nthat require approval.\n').or(zod.enum(['always', 'never']).describe('Specify a single approval policy for all tools. One of `always` or\n`never`. When set to `always`, all tools will require approval. When\nset to `never`, all tools will not require approval.\n')).optional().describe('Specify which of the MCP server\'s tools require approval.').or(zod.null()).optional()
}).describe('Give the model access to additional tools via remote Model Context Protocol\n(MCP) servers. [Learn more about MCP](https://platform.openai.com/docs/guides/tools-remote-mcp).\n')])).optional().describe('Tools available to the model.'),
  "tool_choice": zod.enum(['none', 'auto', 'required']).describe('Controls which (if any) tool is called by the model.\n\n`none` means the model will not call any tool and instead generates a message.\n\n`auto` means the model can pick between generating a message or calling one or\nmore tools.\n\n`required` means the model must call one or more tools.\n').or(zod.object({
  "type": zod.enum(['function']).describe('For function calling, the type is always `function`.'),
  "name": zod.string().describe('The name of the function to call.')
}).describe('Use this option to force the model to call a specific function.\n')).or(zod.object({
  "type": zod.enum(['mcp']).describe('For MCP tools, the type is always `mcp`.'),
  "server_label": zod.string().describe('The label of the MCP server to use.\n'),
  "name": zod.string().describe('The name of the tool to call on the server.\n').or(zod.null()).optional()
}).describe('Use this option to force the model to call a specific tool on a remote MCP server.\n')).optional().describe('How the model chooses tools. Provide one of the string modes or force a specific\nfunction/MCP tool.\n'),
  "max_output_tokens": zod.number().or(zod.enum(['inf'])).optional().describe('Maximum number of output tokens for a single assistant response,\ninclusive of tool calls. Provide an integer between 1 and 4096 to\nlimit output tokens, or `inf` for the maximum available tokens for a\ngiven model. Defaults to `inf`.\n'),
  "truncation": zod.enum(['auto', 'disabled']).describe('The truncation strategy to use for the session. `auto` is the default truncation strategy. `disabled` will disable truncation and emit errors when the conversation exceeds the input token limit.').or(zod.object({
  "type": zod.enum(['retention_ratio']).describe('Use retention ratio truncation.'),
  "retention_ratio": zod.number().min(acceptRealtimeCallBodyTruncationRetentionRatioMin).max(acceptRealtimeCallBodyTruncationRetentionRatioMax).describe('Fraction of post-instruction conversation tokens to retain (0.0 - 1.0) when the conversation exceeds the input token limit.\n')
}).describe('Retain a fraction of the conversation tokens when the conversation exceeds the input token limit. This allows you to amortize truncations across multiple turns, which can help improve cached token usage.')).optional().describe('Controls how the realtime conversation is truncated prior to model inference.\nThe default is `auto`.\n'),
  "prompt": zod.object({
  "id": zod.string().describe('The unique identifier of the prompt template to use.'),
  "version": zod.string().describe('Optional version of the prompt template.').or(zod.null()).optional(),
  "variables": zod.record(zod.string(), zod.string().or(zod.object({
  "type": zod.enum(['input_text']).optional().describe('The type of the input item. Always `input_text`.'),
  "text": zod.string().describe('The text input to the model.')
}).describe('A text input to the model.')).or(zod.object({
  "type": zod.enum(['input_image']).optional().describe('The type of the input item. Always `input_image`.'),
  "image_url": zod.string().describe('The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL.').or(zod.null()).optional(),
  "file_id": zod.string().describe('The ID of the file to be sent to the model.').or(zod.null()).optional(),
  "detail": zod.enum(['low', 'high', 'auto'])
}).describe('An image input to the model. Learn about [image inputs](https://platform.openai.com/docs/guides/vision).')).or(zod.object({
  "type": zod.enum(['input_file']).optional().describe('The type of the input item. Always `input_file`.'),
  "file_id": zod.string().describe('The ID of the file to be sent to the model.').or(zod.null()).optional(),
  "filename": zod.string().optional().describe('The name of the file to be sent to the model.'),
  "file_url": zod.string().optional().describe('The URL of the file to be sent to the model.'),
  "file_data": zod.string().optional().describe('The content of the file to be sent to the model.\n')
}).describe('A file input to the model.'))).describe('Optional map of values to substitute in for variables in your\nprompt. The substitution values can either be strings, or other\nResponse input types like images or files.\n').or(zod.null()).optional()
}).describe('Reference to a prompt template and its variables.\n[Learn more](https://platform.openai.com/docs/guides/text?api-mode=responses#reusable-prompts).\n').or(zod.null()).optional()
}).describe('Realtime session object configuration.')

/**
 * End an active Realtime API call, whether it was initiated over SIP or
WebRTC.
 * @summary Hang up call
 */
export const hangupRealtimeCallParams = zod.object({
  "call_id": zod.string().describe('The identifier for the call. For SIP calls, use the value provided in the\n[`realtime.call.incoming`](https://platform.openai.com/docs/api-reference/webhook_events/realtime/call/incoming)\nwebhook. For WebRTC sessions, reuse the call ID returned in the `Location`\nheader when creating the call with\n[`POST /v1/realtime/calls`](https://platform.openai.com/docs/api-reference/realtime/create-call).')
})

/**
 * Transfer an active SIP call to a new destination using the SIP REFER verb.
 * @summary Refer call
 */
export const referRealtimeCallParams = zod.object({
  "call_id": zod.string().describe('The identifier for the call provided in the\n[`realtime.call.incoming`](https://platform.openai.com/docs/api-reference/webhook_events/realtime/call/incoming)\nwebhook.')
})

export const referRealtimeCallBody = zod.object({
  "target_uri": zod.string().describe('URI that should appear in the SIP Refer-To header. Supports values like\n`tel:+14155550123` or `sip:agent@example.com`.')
}).describe('Parameters required to transfer a SIP call to a new destination using the\nRealtime API.')

/**
 * Decline an incoming SIP call by returning a SIP status code to the caller.
 * @summary Reject call
 */
export const rejectRealtimeCallParams = zod.object({
  "call_id": zod.string().describe('The identifier for the call provided in the\n[`realtime.call.incoming`](https://platform.openai.com/docs/api-reference/webhook_events/realtime/call/incoming)\nwebhook.')
})

export const rejectRealtimeCallBody = zod.object({
  "status_code": zod.number().optional().describe('SIP response code to send back to the caller. Defaults to `603` (Decline)\nwhen omitted.')
}).describe('Parameters used to decline an incoming SIP call handled by the Realtime API.')

/**
 * Create a Realtime client secret with an associated session configuration.

 * @summary Create client secret
 */
export const createRealtimeClientSecretBodyExpiresAfterAnchorDefault = "created_at";export const createRealtimeClientSecretBodyExpiresAfterSecondsDefault = 600;
export const createRealtimeClientSecretBodyExpiresAfterSecondsMin = 10;

export const createRealtimeClientSecretBodyExpiresAfterSecondsMax = 7200;
export const createRealtimeClientSecretBodySessionOutputModalitiesDefault = ["audio"];export const createRealtimeClientSecretBodySessionAudioInputTurnDetectionTypeDefault = "server_vad";export const createRealtimeClientSecretBodySessionAudioInputTurnDetectionCreateResponseDefault = true;export const createRealtimeClientSecretBodySessionAudioInputTurnDetectionInterruptResponseDefault = true;export const createRealtimeClientSecretBodySessionAudioInputTurnDetectionIdleTimeoutMsMinOne = 5000;
export const createRealtimeClientSecretBodySessionAudioInputTurnDetectionIdleTimeoutMsMaxOne = 30000;
export const createRealtimeClientSecretBodySessionAudioInputTurnDetectionEagernessDefault = "auto";export const createRealtimeClientSecretBodySessionAudioInputTurnDetectionCreateResponseDefaultOne = true;export const createRealtimeClientSecretBodySessionAudioInputTurnDetectionInterruptResponseDefaultOne = true;export const createRealtimeClientSecretBodySessionAudioOutputSpeedDefault = 1;
export const createRealtimeClientSecretBodySessionAudioOutputSpeedMin = 0.25;

export const createRealtimeClientSecretBodySessionAudioOutputSpeedMax = 1.5;
export const createRealtimeClientSecretBodySessionTracingDefaultOne = "auto";export const createRealtimeClientSecretBodySessionToolsItemRequireApprovalDefaultOne = "always";export const createRealtimeClientSecretBodySessionToolChoiceDefault = "auto";export const createRealtimeClientSecretBodySessionTruncationRetentionRatioMin = 0;

export const createRealtimeClientSecretBodySessionTruncationRetentionRatioMax = 1;
export const createRealtimeClientSecretBodySessionPromptVariablesTypeDefault = "input_text";export const createRealtimeClientSecretBodySessionPromptVariablesTypeDefaultOne = "input_image";export const createRealtimeClientSecretBodySessionPromptVariablesTypeDefaultTwo = "input_file";export const createRealtimeClientSecretBodySessionAudioInputTurnDetectionTypeDefaultTwo = "server_vad";export const createRealtimeClientSecretBodySessionAudioInputTurnDetectionCreateResponseDefaultTwo = true;export const createRealtimeClientSecretBodySessionAudioInputTurnDetectionInterruptResponseDefaultTwo = true;export const createRealtimeClientSecretBodySessionAudioInputTurnDetectionIdleTimeoutMsMinFour = 5000;
export const createRealtimeClientSecretBodySessionAudioInputTurnDetectionIdleTimeoutMsMaxFour = 30000;
export const createRealtimeClientSecretBodySessionAudioInputTurnDetectionEagernessDefaultOne = "auto";export const createRealtimeClientSecretBodySessionAudioInputTurnDetectionCreateResponseDefaultThree = true;export const createRealtimeClientSecretBodySessionAudioInputTurnDetectionInterruptResponseDefaultThree = true;

export const createRealtimeClientSecretBody = zod.object({
  "expires_after": zod.object({
  "anchor": zod.enum(['created_at']).optional().describe('The anchor point for the client secret expiration, meaning that `seconds` will be added to the `created_at` time of the client secret to produce an expiration timestamp. Only `created_at` is currently supported.\n'),
  "seconds": zod.number().min(createRealtimeClientSecretBodyExpiresAfterSecondsMin).max(createRealtimeClientSecretBodyExpiresAfterSecondsMax).optional().describe('The number of seconds from the anchor point to the expiration. Select a value between `10` and `7200` (2 hours). This default to 600 seconds (10 minutes) if not specified.\n')
}).optional().describe('Configuration for the client secret expiration. Expiration refers to the time after which\na client secret will no longer be valid for creating sessions. The session itself may\ncontinue after that time once started. A secret can be used to create multiple sessions\nuntil it expires.\n'),
  "session": zod.discriminatedUnion('type', [zod.object({
  "type": zod.enum(['realtime']).describe('The type of session to create. Always `realtime` for the Realtime API.\n'),
  "output_modalities": zod.array(zod.enum(['text', 'audio'])).optional().describe('The set of modalities the model can respond with. It defaults to `[\"audio\"]`, indicating\nthat the model will respond with audio plus a transcript. `[\"text\"]` can be used to make\nthe model respond with text only. It is not possible to request both `text` and `audio` at the same time.\n'),
  "model": zod.string().or(zod.enum(['gpt-realtime', 'gpt-realtime-2025-08-28', 'gpt-4o-realtime-preview', 'gpt-4o-realtime-preview-2024-10-01', 'gpt-4o-realtime-preview-2024-12-17', 'gpt-4o-realtime-preview-2025-06-03', 'gpt-4o-mini-realtime-preview', 'gpt-4o-mini-realtime-preview-2024-12-17', 'gpt-realtime-mini', 'gpt-realtime-mini-2025-10-06', 'gpt-audio-mini', 'gpt-audio-mini-2025-10-06'])).optional().describe('The Realtime model used for this session.\n'),
  "instructions": zod.string().optional().describe('The default system instructions (i.e. system message) prepended to model calls. This field allows the client to guide the model on desired responses. The model can be instructed on response content and format, (e.g. \"be extremely succinct\", \"act friendly\", \"here are examples of good responses\") and on audio behavior (e.g. \"talk quickly\", \"inject emotion into your voice\", \"laugh frequently\"). The instructions are not guaranteed to be followed by the model, but they provide guidance to the model on the desired behavior.\n\nNote that the server sets default instructions which will be used if this field is not set and are visible in the `session.created` event at the start of the session.\n'),
  "audio": zod.object({
  "input": zod.object({
  "format": zod.discriminatedUnion('type', [zod.object({
  "type": zod.enum(['audio/pcm']).optional().describe('The audio format. Always `audio/pcm`.'),
  "rate": zod.number().optional().describe('The sample rate of the audio. Always `24000`.')
}).describe('The PCM audio format. Only a 24kHz sample rate is supported.'),zod.object({
  "type": zod.enum(['audio/pcmu']).optional().describe('The audio format. Always `audio/pcmu`.')
}).describe('The G.711 Œº-law format.'),zod.object({
  "type": zod.enum(['audio/pcma']).optional().describe('The audio format. Always `audio/pcma`.')
}).describe('The G.711 A-law format.')]).optional(),
  "transcription": zod.object({
  "model": zod.enum(['whisper-1', 'gpt-4o-mini-transcribe', 'gpt-4o-transcribe', 'gpt-4o-transcribe-diarize']).optional().describe('The model to use for transcription. Current options are `whisper-1`, `gpt-4o-mini-transcribe`, `gpt-4o-transcribe`, and `gpt-4o-transcribe-diarize`. Use `gpt-4o-transcribe-diarize` when you need diarization with speaker labels.\n'),
  "language": zod.string().optional().describe('The language of the input audio. Supplying the input language in\n[ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`) format\nwill improve accuracy and latency.\n'),
  "prompt": zod.string().optional().describe('An optional text to guide the model\'s style or continue a previous audio\nsegment.\nFor `whisper-1`, the [prompt is a list of keywords](https://platform.openai.com/docs/guides/speech-to-text#prompting).\nFor `gpt-4o-transcribe` models (excluding `gpt-4o-transcribe-diarize`), the prompt is a free text string, for example \"expect words related to technology\".\n')
}).optional(),
  "noise_reduction": zod.object({
  "type": zod.enum(['near_field', 'far_field']).optional().describe('Type of noise reduction. `near_field` is for close-talking microphones such as headphones, `far_field` is for far-field microphones such as laptop or conference room microphones.\n')
}).optional().describe('Configuration for input audio noise reduction. This can be set to `null` to turn off.\nNoise reduction filters audio added to the input audio buffer before it is sent to VAD and the model.\nFiltering the audio can improve VAD and turn detection accuracy (reducing false positives) and model performance by improving perception of the input audio.\n'),
  "turn_detection": zod.discriminatedUnion('type', [zod.object({
  "type": zod.string().optional().describe('Type of turn detection, `server_vad` to turn on simple Server VAD.\n'),
  "threshold": zod.number().optional().describe('Used only for `server_vad` mode. Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A\nhigher threshold will require louder audio to activate the model, and\nthus might perform better in noisy environments.\n'),
  "prefix_padding_ms": zod.number().optional().describe('Used only for `server_vad` mode. Amount of audio to include before the VAD detected speech (in\nmilliseconds). Defaults to 300ms.\n'),
  "silence_duration_ms": zod.number().optional().describe('Used only for `server_vad` mode. Duration of silence to detect speech stop (in milliseconds). Defaults\nto 500ms. With shorter values the model will respond more quickly,\nbut may jump in on short pauses from the user.\n'),
  "create_response": zod.boolean().optional().describe('Whether or not to automatically generate a response when a VAD stop event occurs.\n'),
  "interrupt_response": zod.boolean().optional().describe('Whether or not to automatically interrupt any ongoing response with output to the default\nconversation (i.e. `conversation` of `auto`) when a VAD start event occurs.\n'),
  "idle_timeout_ms": zod.number().min(createRealtimeClientSecretBodySessionAudioInputTurnDetectionIdleTimeoutMsMinOne).max(createRealtimeClientSecretBodySessionAudioInputTurnDetectionIdleTimeoutMsMaxOne).describe('Optional timeout after which a model response will be triggered automatically. This is\nuseful for situations in which a long pause from the user is unexpected, such as a phone\ncall. The model will effectively prompt the user to continue the conversation based\non the current context.\n\nThe timeout value will be applied after the last model response\'s audio has finished playing,\ni.e. it\'s set to the `response.done` time plus audio playback duration.\n\nAn `input_audio_buffer.timeout_triggered` event (plus events\nassociated with the Response) will be emitted when the timeout is reached.\nIdle timeout is currently only supported for `server_vad` mode.\n').or(zod.null()).optional()
}).describe('Server-side voice activity detection (VAD) which flips on when user speech is detected and off after a period of silence.'),zod.object({
  "type": zod.string().describe('Type of turn detection, `semantic_vad` to turn on Semantic VAD.\n'),
  "eagerness": zod.enum(['low', 'medium', 'high', 'auto']).optional().describe('Used only for `semantic_vad` mode. The eagerness of the model to respond. `low` will wait longer for the user to continue speaking, `high` will respond more quickly. `auto` is the default and is equivalent to `medium`. `low`, `medium`, and `high` have max timeouts of 8s, 4s, and 2s respectively.\n'),
  "create_response": zod.boolean().optional().describe('Whether or not to automatically generate a response when a VAD stop event occurs.\n'),
  "interrupt_response": zod.boolean().optional().describe('Whether or not to automatically interrupt any ongoing response with output to the default\nconversation (i.e. `conversation` of `auto`) when a VAD start event occurs.\n')
}).describe('Server-side semantic turn detection which uses a model to determine when the user has finished speaking.')]).describe('Configuration for turn detection, ether Server VAD or Semantic VAD. This can be set to `null` to turn off, in which case the client must manually trigger model response.\n\nServer VAD means that the model will detect the start and end of speech based on audio volume and respond at the end of user speech.\n\nSemantic VAD is more advanced and uses a turn detection model (in conjunction with VAD) to semantically estimate whether the user has finished speaking, then dynamically sets a timeout based on this probability. For example, if user audio trails off with \"uhhm\", the model will score a low probability of turn end and wait longer for the user to continue speaking. This can be useful for more natural conversations, but may have a higher latency.\n').or(zod.null()).optional()
}).optional(),
  "output": zod.object({
  "format": zod.discriminatedUnion('type', [zod.object({
  "type": zod.enum(['audio/pcm']).optional().describe('The audio format. Always `audio/pcm`.'),
  "rate": zod.number().optional().describe('The sample rate of the audio. Always `24000`.')
}).describe('The PCM audio format. Only a 24kHz sample rate is supported.'),zod.object({
  "type": zod.enum(['audio/pcmu']).optional().describe('The audio format. Always `audio/pcmu`.')
}).describe('The G.711 Œº-law format.'),zod.object({
  "type": zod.enum(['audio/pcma']).optional().describe('The audio format. Always `audio/pcma`.')
}).describe('The G.711 A-law format.')]).optional(),
  "voice": zod.string().or(zod.enum(['alloy', 'ash', 'ballad', 'coral', 'echo', 'sage', 'shimmer', 'verse', 'marin', 'cedar'])).optional(),
  "speed": zod.number().min(createRealtimeClientSecretBodySessionAudioOutputSpeedMin).max(createRealtimeClientSecretBodySessionAudioOutputSpeedMax).optional().describe('The speed of the model\'s spoken response as a multiple of the original speed.\n1.0 is the default speed. 0.25 is the minimum speed. 1.5 is the maximum speed. This value can only be changed in between model turns, not while a response is in progress.\n\nThis parameter is a post-processing adjustment to the audio after it is generated, it\'s\nalso possible to prompt the model to speak faster or slower.\n')
}).optional()
}).optional().describe('Configuration for input and output audio.\n'),
  "include": zod.array(zod.enum(['item.input_audio_transcription.logprobs'])).optional().describe('Additional fields to include in server outputs.\n\n`item.input_audio_transcription.logprobs`: Include logprobs for input audio transcription.\n'),
  "tracing": zod.enum(['auto']).optional().describe('Default tracing mode for the session.\n').or(zod.object({
  "workflow_name": zod.string().optional().describe('The name of the workflow to attach to this trace. This is used to\nname the trace in the Traces Dashboard.\n'),
  "group_id": zod.string().optional().describe('The group id to attach to this trace to enable filtering and\ngrouping in the Traces Dashboard.\n'),
  "metadata": zod.object({

}).optional().describe('The arbitrary metadata to attach to this trace to enable\nfiltering in the Traces Dashboard.\n')
}).describe('Granular configuration for tracing.\n')).nullish().describe('Realtime API can write session traces to the [Traces Dashboard](/logs?api=traces). Set to null to disable tracing. Once\ntracing is enabled for a session, the configuration cannot be modified.\n\n`auto` will create a trace for the session with default values for the\nworkflow name, group id, and metadata.\n'),
  "tools": zod.array(zod.discriminatedUnion('type', [zod.object({
  "type": zod.enum(['function']).optional().describe('The type of the tool, i.e. `function`.'),
  "name": zod.string().optional().describe('The name of the function.'),
  "description": zod.string().optional().describe('The description of the function, including guidance on when and how\nto call it, and guidance about what to tell the user when calling\n(if anything).\n'),
  "parameters": zod.object({

}).optional().describe('Parameters of the function in JSON Schema.')
}),zod.object({
  "type": zod.enum(['mcp']).describe('The type of the MCP tool. Always `mcp`.'),
  "server_label": zod.string().describe('A label for this MCP server, used to identify it in tool calls.\n'),
  "server_url": zod.string().optional().describe('The URL for the MCP server. One of `server_url` or `connector_id` must be\nprovided.\n'),
  "connector_id": zod.enum(['connector_dropbox', 'connector_gmail', 'connector_googlecalendar', 'connector_googledrive', 'connector_microsoftteams', 'connector_outlookcalendar', 'connector_outlookemail', 'connector_sharepoint']).optional().describe('Identifier for service connectors, like those available in ChatGPT. One of\n`server_url` or `connector_id` must be provided. Learn more about service\nconnectors [here](https://platform.openai.com/docs/guides/tools-remote-mcp#connectors).\n\nCurrently supported `connector_id` values are:\n\n- Dropbox: `connector_dropbox`\n- Gmail: `connector_gmail`\n- Google Calendar: `connector_googlecalendar`\n- Google Drive: `connector_googledrive`\n- Microsoft Teams: `connector_microsoftteams`\n- Outlook Calendar: `connector_outlookcalendar`\n- Outlook Email: `connector_outlookemail`\n- SharePoint: `connector_sharepoint`\n'),
  "authorization": zod.string().optional().describe('An OAuth access token that can be used with a remote MCP server, either\nwith a custom MCP server URL or a service connector. Your application\nmust handle the OAuth authorization flow and provide the token here.\n'),
  "server_description": zod.string().optional().describe('Optional description of the MCP server, used to provide more context.\n'),
  "headers": zod.record(zod.string(), zod.string()).describe('Optional HTTP headers to send to the MCP server. Use for authentication\nor other purposes.\n').or(zod.null()).optional(),
  "allowed_tools": zod.array(zod.string()).describe('A string array of allowed tool names').or(zod.object({
  "tool_names": zod.array(zod.string()).optional().describe('List of allowed tool names.'),
  "read_only": zod.boolean().optional().describe('Indicates whether or not a tool modifies data or is read-only. If an\nMCP server is [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\nit will match this filter.\n')
}).describe('A filter object to specify which tools are allowed.\n')).describe('List of allowed tool names or a filter object.\n').or(zod.null()).optional(),
  "require_approval": zod.object({
  "always": zod.object({
  "tool_names": zod.array(zod.string()).optional().describe('List of allowed tool names.'),
  "read_only": zod.boolean().optional().describe('Indicates whether or not a tool modifies data or is read-only. If an\nMCP server is [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\nit will match this filter.\n')
}).optional().describe('A filter object to specify which tools are allowed.\n'),
  "never": zod.object({
  "tool_names": zod.array(zod.string()).optional().describe('List of allowed tool names.'),
  "read_only": zod.boolean().optional().describe('Indicates whether or not a tool modifies data or is read-only. If an\nMCP server is [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\nit will match this filter.\n')
}).optional().describe('A filter object to specify which tools are allowed.\n')
}).describe('Specify which of the MCP server\'s tools require approval. Can be\n`always`, `never`, or a filter object associated with tools\nthat require approval.\n').or(zod.enum(['always', 'never']).describe('Specify a single approval policy for all tools. One of `always` or\n`never`. When set to `always`, all tools will require approval. When\nset to `never`, all tools will not require approval.\n')).optional().describe('Specify which of the MCP server\'s tools require approval.').or(zod.null()).optional()
}).describe('Give the model access to additional tools via remote Model Context Protocol\n(MCP) servers. [Learn more about MCP](https://platform.openai.com/docs/guides/tools-remote-mcp).\n')])).optional().describe('Tools available to the model.'),
  "tool_choice": zod.enum(['none', 'auto', 'required']).describe('Controls which (if any) tool is called by the model.\n\n`none` means the model will not call any tool and instead generates a message.\n\n`auto` means the model can pick between generating a message or calling one or\nmore tools.\n\n`required` means the model must call one or more tools.\n').or(zod.object({
  "type": zod.enum(['function']).describe('For function calling, the type is always `function`.'),
  "name": zod.string().describe('The name of the function to call.')
}).describe('Use this option to force the model to call a specific function.\n')).or(zod.object({
  "type": zod.enum(['mcp']).describe('For MCP tools, the type is always `mcp`.'),
  "server_label": zod.string().describe('The label of the MCP server to use.\n'),
  "name": zod.string().describe('The name of the tool to call on the server.\n').or(zod.null()).optional()
}).describe('Use this option to force the model to call a specific tool on a remote MCP server.\n')).optional().describe('How the model chooses tools. Provide one of the string modes or force a specific\nfunction/MCP tool.\n'),
  "max_output_tokens": zod.number().or(zod.enum(['inf'])).optional().describe('Maximum number of output tokens for a single assistant response,\ninclusive of tool calls. Provide an integer between 1 and 4096 to\nlimit output tokens, or `inf` for the maximum available tokens for a\ngiven model. Defaults to `inf`.\n'),
  "truncation": zod.enum(['auto', 'disabled']).describe('The truncation strategy to use for the session. `auto` is the default truncation strategy. `disabled` will disable truncation and emit errors when the conversation exceeds the input token limit.').or(zod.object({
  "type": zod.enum(['retention_ratio']).describe('Use retention ratio truncation.'),
  "retention_ratio": zod.number().min(createRealtimeClientSecretBodySessionTruncationRetentionRatioMin).max(createRealtimeClientSecretBodySessionTruncationRetentionRatioMax).describe('Fraction of post-instruction conversation tokens to retain (0.0 - 1.0) when the conversation exceeds the input token limit.\n')
}).describe('Retain a fraction of the conversation tokens when the conversation exceeds the input token limit. This allows you to amortize truncations across multiple turns, which can help improve cached token usage.')).optional().describe('Controls how the realtime conversation is truncated prior to model inference.\nThe default is `auto`.\n'),
  "prompt": zod.object({
  "id": zod.string().describe('The unique identifier of the prompt template to use.'),
  "version": zod.string().describe('Optional version of the prompt template.').or(zod.null()).optional(),
  "variables": zod.record(zod.string(), zod.string().or(zod.object({
  "type": zod.enum(['input_text']).optional().describe('The type of the input item. Always `input_text`.'),
  "text": zod.string().describe('The text input to the model.')
}).describe('A text input to the model.')).or(zod.object({
  "type": zod.enum(['input_image']).optional().describe('The type of the input item. Always `input_image`.'),
  "image_url": zod.string().describe('The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL.').or(zod.null()).optional(),
  "file_id": zod.string().describe('The ID of the file to be sent to the model.').or(zod.null()).optional(),
  "detail": zod.enum(['low', 'high', 'auto'])
}).describe('An image input to the model. Learn about [image inputs](https://platform.openai.com/docs/guides/vision).')).or(zod.object({
  "type": zod.enum(['input_file']).optional().describe('The type of the input item. Always `input_file`.'),
  "file_id": zod.string().describe('The ID of the file to be sent to the model.').or(zod.null()).optional(),
  "filename": zod.string().optional().describe('The name of the file to be sent to the model.'),
  "file_url": zod.string().optional().describe('The URL of the file to be sent to the model.'),
  "file_data": zod.string().optional().describe('The content of the file to be sent to the model.\n')
}).describe('A file input to the model.'))).describe('Optional map of values to substitute in for variables in your\nprompt. The substitution values can either be strings, or other\nResponse input types like images or files.\n').or(zod.null()).optional()
}).describe('Reference to a prompt template and its variables.\n[Learn more](https://platform.openai.com/docs/guides/text?api-mode=responses#reusable-prompts).\n').or(zod.null()).optional()
}).describe('Realtime session object configuration.'),zod.object({
  "type": zod.enum(['transcription']).describe('The type of session to create. Always `transcription` for transcription sessions.\n'),
  "audio": zod.object({
  "input": zod.object({
  "format": zod.discriminatedUnion('type', [zod.object({
  "type": zod.enum(['audio/pcm']).optional().describe('The audio format. Always `audio/pcm`.'),
  "rate": zod.number().optional().describe('The sample rate of the audio. Always `24000`.')
}).describe('The PCM audio format. Only a 24kHz sample rate is supported.'),zod.object({
  "type": zod.enum(['audio/pcmu']).optional().describe('The audio format. Always `audio/pcmu`.')
}).describe('The G.711 Œº-law format.'),zod.object({
  "type": zod.enum(['audio/pcma']).optional().describe('The audio format. Always `audio/pcma`.')
}).describe('The G.711 A-law format.')]).optional(),
  "transcription": zod.object({
  "model": zod.enum(['whisper-1', 'gpt-4o-mini-transcribe', 'gpt-4o-transcribe', 'gpt-4o-transcribe-diarize']).optional().describe('The model to use for transcription. Current options are `whisper-1`, `gpt-4o-mini-transcribe`, `gpt-4o-transcribe`, and `gpt-4o-transcribe-diarize`. Use `gpt-4o-transcribe-diarize` when you need diarization with speaker labels.\n'),
  "language": zod.string().optional().describe('The language of the input audio. Supplying the input language in\n[ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`) format\nwill improve accuracy and latency.\n'),
  "prompt": zod.string().optional().describe('An optional text to guide the model\'s style or continue a previous audio\nsegment.\nFor `whisper-1`, the [prompt is a list of keywords](https://platform.openai.com/docs/guides/speech-to-text#prompting).\nFor `gpt-4o-transcribe` models (excluding `gpt-4o-transcribe-diarize`), the prompt is a free text string, for example \"expect words related to technology\".\n')
}).optional(),
  "noise_reduction": zod.object({
  "type": zod.enum(['near_field', 'far_field']).optional().describe('Type of noise reduction. `near_field` is for close-talking microphones such as headphones, `far_field` is for far-field microphones such as laptop or conference room microphones.\n')
}).optional().describe('Configuration for input audio noise reduction. This can be set to `null` to turn off.\nNoise reduction filters audio added to the input audio buffer before it is sent to VAD and the model.\nFiltering the audio can improve VAD and turn detection accuracy (reducing false positives) and model performance by improving perception of the input audio.\n'),
  "turn_detection": zod.discriminatedUnion('type', [zod.object({
  "type": zod.string().optional().describe('Type of turn detection, `server_vad` to turn on simple Server VAD.\n'),
  "threshold": zod.number().optional().describe('Used only for `server_vad` mode. Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A\nhigher threshold will require louder audio to activate the model, and\nthus might perform better in noisy environments.\n'),
  "prefix_padding_ms": zod.number().optional().describe('Used only for `server_vad` mode. Amount of audio to include before the VAD detected speech (in\nmilliseconds). Defaults to 300ms.\n'),
  "silence_duration_ms": zod.number().optional().describe('Used only for `server_vad` mode. Duration of silence to detect speech stop (in milliseconds). Defaults\nto 500ms. With shorter values the model will respond more quickly,\nbut may jump in on short pauses from the user.\n'),
  "create_response": zod.boolean().optional().describe('Whether or not to automatically generate a response when a VAD stop event occurs.\n'),
  "interrupt_response": zod.boolean().optional().describe('Whether or not to automatically interrupt any ongoing response with output to the default\nconversation (i.e. `conversation` of `auto`) when a VAD start event occurs.\n'),
  "idle_timeout_ms": zod.number().min(createRealtimeClientSecretBodySessionAudioInputTurnDetectionIdleTimeoutMsMinFour).max(createRealtimeClientSecretBodySessionAudioInputTurnDetectionIdleTimeoutMsMaxFour).describe('Optional timeout after which a model response will be triggered automatically. This is\nuseful for situations in which a long pause from the user is unexpected, such as a phone\ncall. The model will effectively prompt the user to continue the conversation based\non the current context.\n\nThe timeout value will be applied after the last model response\'s audio has finished playing,\ni.e. it\'s set to the `response.done` time plus audio playback duration.\n\nAn `input_audio_buffer.timeout_triggered` event (plus events\nassociated with the Response) will be emitted when the timeout is reached.\nIdle timeout is currently only supported for `server_vad` mode.\n').or(zod.null()).optional()
}).describe('Server-side voice activity detection (VAD) which flips on when user speech is detected and off after a period of silence.'),zod.object({
  "type": zod.string().describe('Type of turn detection, `semantic_vad` to turn on Semantic VAD.\n'),
  "eagerness": zod.enum(['low', 'medium', 'high', 'auto']).optional().describe('Used only for `semantic_vad` mode. The eagerness of the model to respond. `low` will wait longer for the user to continue speaking, `high` will respond more quickly. `auto` is the default and is equivalent to `medium`. `low`, `medium`, and `high` have max timeouts of 8s, 4s, and 2s respectively.\n'),
  "create_response": zod.boolean().optional().describe('Whether or not to automatically generate a response when a VAD stop event occurs.\n'),
  "interrupt_response": zod.boolean().optional().describe('Whether or not to automatically interrupt any ongoing response with output to the default\nconversation (i.e. `conversation` of `auto`) when a VAD start event occurs.\n')
}).describe('Server-side semantic turn detection which uses a model to determine when the user has finished speaking.')]).describe('Configuration for turn detection, ether Server VAD or Semantic VAD. This can be set to `null` to turn off, in which case the client must manually trigger model response.\n\nServer VAD means that the model will detect the start and end of speech based on audio volume and respond at the end of user speech.\n\nSemantic VAD is more advanced and uses a turn detection model (in conjunction with VAD) to semantically estimate whether the user has finished speaking, then dynamically sets a timeout based on this probability. For example, if user audio trails off with \"uhhm\", the model will score a low probability of turn end and wait longer for the user to continue speaking. This can be useful for more natural conversations, but may have a higher latency.\n').or(zod.null()).optional()
}).optional()
}).optional().describe('Configuration for input and output audio.\n'),
  "include": zod.array(zod.enum(['item.input_audio_transcription.logprobs'])).optional().describe('Additional fields to include in server outputs.\n\n`item.input_audio_transcription.logprobs`: Include logprobs for input audio transcription.\n')
}).describe('Realtime transcription session object configuration.')]).optional().describe('Session configuration to use for the client secret. Choose either a realtime\nsession or a transcription session.\n')
}).describe('Create a session and client secret for the Realtime API. The request can specify\neither a realtime or a transcription session configuration.\n[Learn more about the Realtime API](https://platform.openai.com/docs/guides/realtime).\n')

export const createRealtimeClientSecretResponseSessionOutputModalitiesDefault = ["audio"];export const createRealtimeClientSecretResponseSessionAudioInputTurnDetectionTypeDefault = "server_vad";export const createRealtimeClientSecretResponseSessionAudioInputTurnDetectionCreateResponseDefault = true;export const createRealtimeClientSecretResponseSessionAudioInputTurnDetectionInterruptResponseDefault = true;export const createRealtimeClientSecretResponseSessionAudioInputTurnDetectionIdleTimeoutMsMinOne = 5000;
export const createRealtimeClientSecretResponseSessionAudioInputTurnDetectionIdleTimeoutMsMaxOne = 30000;
export const createRealtimeClientSecretResponseSessionAudioInputTurnDetectionEagernessDefault = "auto";export const createRealtimeClientSecretResponseSessionAudioInputTurnDetectionCreateResponseDefaultOne = true;export const createRealtimeClientSecretResponseSessionAudioInputTurnDetectionInterruptResponseDefaultOne = true;export const createRealtimeClientSecretResponseSessionAudioOutputSpeedDefault = 1;
export const createRealtimeClientSecretResponseSessionAudioOutputSpeedMin = 0.25;

export const createRealtimeClientSecretResponseSessionAudioOutputSpeedMax = 1.5;
export const createRealtimeClientSecretResponseSessionTracingDefaultTwo = "auto";export const createRealtimeClientSecretResponseSessionToolsItemRequireApprovalDefaultOne = "always";export const createRealtimeClientSecretResponseSessionToolChoiceDefault = "auto";export const createRealtimeClientSecretResponseSessionTruncationRetentionRatioMin = 0;

export const createRealtimeClientSecretResponseSessionTruncationRetentionRatioMax = 1;
export const createRealtimeClientSecretResponseSessionPromptVariablesTypeDefault = "input_text";export const createRealtimeClientSecretResponseSessionPromptVariablesTypeDefaultOne = "input_image";export const createRealtimeClientSecretResponseSessionPromptVariablesTypeDefaultTwo = "input_file";

export const createRealtimeClientSecretResponse = zod.object({
  "value": zod.string().describe('The generated client secret value.'),
  "expires_at": zod.number().describe('Expiration timestamp for the client secret, in seconds since epoch.'),
  "session": zod.discriminatedUnion('type', [zod.object({
  "client_secret": zod.object({
  "value": zod.string().describe('Ephemeral key usable in client environments to authenticate connections to the Realtime API. Use this in client-side environments rather than a standard API token, which should only be used server-side.\n'),
  "expires_at": zod.number().describe('Timestamp for when the token expires. Currently, all tokens expire\nafter one minute.\n')
}).describe('Ephemeral key returned by the API.'),
  "type": zod.enum(['realtime']).describe('The type of session to create. Always `realtime` for the Realtime API.\n'),
  "output_modalities": zod.array(zod.enum(['text', 'audio'])).optional().describe('The set of modalities the model can respond with. It defaults to `[\"audio\"]`, indicating\nthat the model will respond with audio plus a transcript. `[\"text\"]` can be used to make\nthe model respond with text only. It is not possible to request both `text` and `audio` at the same time.\n'),
  "model": zod.string().or(zod.enum(['gpt-realtime', 'gpt-realtime-2025-08-28', 'gpt-4o-realtime-preview', 'gpt-4o-realtime-preview-2024-10-01', 'gpt-4o-realtime-preview-2024-12-17', 'gpt-4o-realtime-preview-2025-06-03', 'gpt-4o-mini-realtime-preview', 'gpt-4o-mini-realtime-preview-2024-12-17', 'gpt-realtime-mini', 'gpt-realtime-mini-2025-10-06', 'gpt-audio-mini', 'gpt-audio-mini-2025-10-06'])).optional().describe('The Realtime model used for this session.\n'),
  "instructions": zod.string().optional().describe('The default system instructions (i.e. system message) prepended to model calls. This field allows the client to guide the model on desired responses. The model can be instructed on response content and format, (e.g. \"be extremely succinct\", \"act friendly\", \"here are examples of good responses\") and on audio behavior (e.g. \"talk quickly\", \"inject emotion into your voice\", \"laugh frequently\"). The instructions are not guaranteed to be followed by the model, but they provide guidance to the model on the desired behavior.\n\nNote that the server sets default instructions which will be used if this field is not set and are visible in the `session.created` event at the start of the session.\n'),
  "audio": zod.object({
  "input": zod.object({
  "format": zod.discriminatedUnion('type', [zod.object({
  "type": zod.enum(['audio/pcm']).optional().describe('The audio format. Always `audio/pcm`.'),
  "rate": zod.number().optional().describe('The sample rate of the audio. Always `24000`.')
}).describe('The PCM audio format. Only a 24kHz sample rate is supported.'),zod.object({
  "type": zod.enum(['audio/pcmu']).optional().describe('The audio format. Always `audio/pcmu`.')
}).describe('The G.711 Œº-law format.'),zod.object({
  "type": zod.enum(['audio/pcma']).optional().describe('The audio format. Always `audio/pcma`.')
}).describe('The G.711 A-law format.')]).optional(),
  "transcription": zod.object({
  "model": zod.enum(['whisper-1', 'gpt-4o-mini-transcribe', 'gpt-4o-transcribe', 'gpt-4o-transcribe-diarize']).optional().describe('The model to use for transcription. Current options are `whisper-1`, `gpt-4o-mini-transcribe`, `gpt-4o-transcribe`, and `gpt-4o-transcribe-diarize`. Use `gpt-4o-transcribe-diarize` when you need diarization with speaker labels.\n'),
  "language": zod.string().optional().describe('The language of the input audio. Supplying the input language in\n[ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`) format\nwill improve accuracy and latency.\n'),
  "prompt": zod.string().optional().describe('An optional text to guide the model\'s style or continue a previous audio\nsegment.\nFor `whisper-1`, the [prompt is a list of keywords](https://platform.openai.com/docs/guides/speech-to-text#prompting).\nFor `gpt-4o-transcribe` models (excluding `gpt-4o-transcribe-diarize`), the prompt is a free text string, for example \"expect words related to technology\".\n')
}).optional(),
  "noise_reduction": zod.object({
  "type": zod.enum(['near_field', 'far_field']).optional().describe('Type of noise reduction. `near_field` is for close-talking microphones such as headphones, `far_field` is for far-field microphones such as laptop or conference room microphones.\n')
}).optional().describe('Configuration for input audio noise reduction. This can be set to `null` to turn off.\nNoise reduction filters audio added to the input audio buffer before it is sent to VAD and the model.\nFiltering the audio can improve VAD and turn detection accuracy (reducing false positives) and model performance by improving perception of the input audio.\n'),
  "turn_detection": zod.discriminatedUnion('type', [zod.object({
  "type": zod.string().optional().describe('Type of turn detection, `server_vad` to turn on simple Server VAD.\n'),
  "threshold": zod.number().optional().describe('Used only for `server_vad` mode. Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A\nhigher threshold will require louder audio to activate the model, and\nthus might perform better in noisy environments.\n'),
  "prefix_padding_ms": zod.number().optional().describe('Used only for `server_vad` mode. Amount of audio to include before the VAD detected speech (in\nmilliseconds). Defaults to 300ms.\n'),
  "silence_duration_ms": zod.number().optional().describe('Used only for `server_vad` mode. Duration of silence to detect speech stop (in milliseconds). Defaults\nto 500ms. With shorter values the model will respond more quickly,\nbut may jump in on short pauses from the user.\n'),
  "create_response": zod.boolean().optional().describe('Whether or not to automatically generate a response when a VAD stop event occurs.\n'),
  "interrupt_response": zod.boolean().optional().describe('Whether or not to automatically interrupt any ongoing response with output to the default\nconversation (i.e. `conversation` of `auto`) when a VAD start event occurs.\n'),
  "idle_timeout_ms": zod.number().min(createRealtimeClientSecretResponseSessionAudioInputTurnDetectionIdleTimeoutMsMinOne).max(createRealtimeClientSecretResponseSessionAudioInputTurnDetectionIdleTimeoutMsMaxOne).describe('Optional timeout after which a model response will be triggered automatically. This is\nuseful for situations in which a long pause from the user is unexpected, such as a phone\ncall. The model will effectively prompt the user to continue the conversation based\non the current context.\n\nThe timeout value will be applied after the last model response\'s audio has finished playing,\ni.e. it\'s set to the `response.done` time plus audio playback duration.\n\nAn `input_audio_buffer.timeout_triggered` event (plus events\nassociated with the Response) will be emitted when the timeout is reached.\nIdle timeout is currently only supported for `server_vad` mode.\n').or(zod.null()).optional()
}).describe('Server-side voice activity detection (VAD) which flips on when user speech is detected and off after a period of silence.'),zod.object({
  "type": zod.string().describe('Type of turn detection, `semantic_vad` to turn on Semantic VAD.\n'),
  "eagerness": zod.enum(['low', 'medium', 'high', 'auto']).optional().describe('Used only for `semantic_vad` mode. The eagerness of the model to respond. `low` will wait longer for the user to continue speaking, `high` will respond more quickly. `auto` is the default and is equivalent to `medium`. `low`, `medium`, and `high` have max timeouts of 8s, 4s, and 2s respectively.\n'),
  "create_response": zod.boolean().optional().describe('Whether or not to automatically generate a response when a VAD stop event occurs.\n'),
  "interrupt_response": zod.boolean().optional().describe('Whether or not to automatically interrupt any ongoing response with output to the default\nconversation (i.e. `conversation` of `auto`) when a VAD start event occurs.\n')
}).describe('Server-side semantic turn detection which uses a model to determine when the user has finished speaking.')]).describe('Configuration for turn detection, ether Server VAD or Semantic VAD. This can be set to `null` to turn off, in which case the client must manually trigger model response.\n\nServer VAD means that the model will detect the start and end of speech based on audio volume and respond at the end of user speech.\n\nSemantic VAD is more advanced and uses a turn detection model (in conjunction with VAD) to semantically estimate whether the user has finished speaking, then dynamically sets a timeout based on this probability. For example, if user audio trails off with \"uhhm\", the model will score a low probability of turn end and wait longer for the user to continue speaking. This can be useful for more natural conversations, but may have a higher latency.\n').or(zod.null()).optional()
}).optional(),
  "output": zod.object({
  "format": zod.discriminatedUnion('type', [zod.object({
  "type": zod.enum(['audio/pcm']).optional().describe('The audio format. Always `audio/pcm`.'),
  "rate": zod.number().optional().describe('The sample rate of the audio. Always `24000`.')
}).describe('The PCM audio format. Only a 24kHz sample rate is supported.'),zod.object({
  "type": zod.enum(['audio/pcmu']).optional().describe('The audio format. Always `audio/pcmu`.')
}).describe('The G.711 Œº-law format.'),zod.object({
  "type": zod.enum(['audio/pcma']).optional().describe('The audio format. Always `audio/pcma`.')
}).describe('The G.711 A-law format.')]).optional(),
  "voice": zod.string().or(zod.enum(['alloy', 'ash', 'ballad', 'coral', 'echo', 'sage', 'shimmer', 'verse', 'marin', 'cedar'])).optional(),
  "speed": zod.number().min(createRealtimeClientSecretResponseSessionAudioOutputSpeedMin).max(createRealtimeClientSecretResponseSessionAudioOutputSpeedMax).optional().describe('The speed of the model\'s spoken response as a multiple of the original speed.\n1.0 is the default speed. 0.25 is the minimum speed. 1.5 is the maximum speed. This value can only be changed in between model turns, not while a response is in progress.\n\nThis parameter is a post-processing adjustment to the audio after it is generated, it\'s\nalso possible to prompt the model to speak faster or slower.\n')
}).optional()
}).optional().describe('Configuration for input and output audio.\n'),
  "include": zod.array(zod.enum(['item.input_audio_transcription.logprobs'])).optional().describe('Additional fields to include in server outputs.\n\n`item.input_audio_transcription.logprobs`: Include logprobs for input audio transcription.\n'),
  "tracing": zod.enum(['auto']).optional().describe('Default tracing mode for the session.\n').or(zod.object({
  "workflow_name": zod.string().optional().describe('The name of the workflow to attach to this trace. This is used to\nname the trace in the Traces Dashboard.\n'),
  "group_id": zod.string().optional().describe('The group id to attach to this trace to enable filtering and\ngrouping in the Traces Dashboard.\n'),
  "metadata": zod.object({

}).optional().describe('The arbitrary metadata to attach to this trace to enable\nfiltering in the Traces Dashboard.\n')
}).describe('Granular configuration for tracing.\n')).describe('Realtime API can write session traces to the [Traces Dashboard](/logs?api=traces). Set to null to disable tracing. Once\ntracing is enabled for a session, the configuration cannot be modified.\n\n`auto` will create a trace for the session with default values for the\nworkflow name, group id, and metadata.\n').or(zod.null()).optional(),
  "tools": zod.array(zod.object({
  "type": zod.enum(['function']).optional().describe('The type of the tool, i.e. `function`.'),
  "name": zod.string().optional().describe('The name of the function.'),
  "description": zod.string().optional().describe('The description of the function, including guidance on when and how\nto call it, and guidance about what to tell the user when calling\n(if anything).\n'),
  "parameters": zod.object({

}).optional().describe('Parameters of the function in JSON Schema.')
}).or(zod.object({
  "type": zod.enum(['mcp']).describe('The type of the MCP tool. Always `mcp`.'),
  "server_label": zod.string().describe('A label for this MCP server, used to identify it in tool calls.\n'),
  "server_url": zod.string().optional().describe('The URL for the MCP server. One of `server_url` or `connector_id` must be\nprovided.\n'),
  "connector_id": zod.enum(['connector_dropbox', 'connector_gmail', 'connector_googlecalendar', 'connector_googledrive', 'connector_microsoftteams', 'connector_outlookcalendar', 'connector_outlookemail', 'connector_sharepoint']).optional().describe('Identifier for service connectors, like those available in ChatGPT. One of\n`server_url` or `connector_id` must be provided. Learn more about service\nconnectors [here](https://platform.openai.com/docs/guides/tools-remote-mcp#connectors).\n\nCurrently supported `connector_id` values are:\n\n- Dropbox: `connector_dropbox`\n- Gmail: `connector_gmail`\n- Google Calendar: `connector_googlecalendar`\n- Google Drive: `connector_googledrive`\n- Microsoft Teams: `connector_microsoftteams`\n- Outlook Calendar: `connector_outlookcalendar`\n- Outlook Email: `connector_outlookemail`\n- SharePoint: `connector_sharepoint`\n'),
  "authorization": zod.string().optional().describe('An OAuth access token that can be used with a remote MCP server, either\nwith a custom MCP server URL or a service connector. Your application\nmust handle the OAuth authorization flow and provide the token here.\n'),
  "server_description": zod.string().optional().describe('Optional description of the MCP server, used to provide more context.\n'),
  "headers": zod.record(zod.string(), zod.string()).describe('Optional HTTP headers to send to the MCP server. Use for authentication\nor other purposes.\n').or(zod.null()).optional(),
  "allowed_tools": zod.array(zod.string()).describe('A string array of allowed tool names').or(zod.object({
  "tool_names": zod.array(zod.string()).optional().describe('List of allowed tool names.'),
  "read_only": zod.boolean().optional().describe('Indicates whether or not a tool modifies data or is read-only. If an\nMCP server is [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\nit will match this filter.\n')
}).describe('A filter object to specify which tools are allowed.\n')).describe('List of allowed tool names or a filter object.\n').or(zod.null()).optional(),
  "require_approval": zod.object({
  "always": zod.object({
  "tool_names": zod.array(zod.string()).optional().describe('List of allowed tool names.'),
  "read_only": zod.boolean().optional().describe('Indicates whether or not a tool modifies data or is read-only. If an\nMCP server is [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\nit will match this filter.\n')
}).optional().describe('A filter object to specify which tools are allowed.\n'),
  "never": zod.object({
  "tool_names": zod.array(zod.string()).optional().describe('List of allowed tool names.'),
  "read_only": zod.boolean().optional().describe('Indicates whether or not a tool modifies data or is read-only. If an\nMCP server is [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\nit will match this filter.\n')
}).optional().describe('A filter object to specify which tools are allowed.\n')
}).describe('Specify which of the MCP server\'s tools require approval. Can be\n`always`, `never`, or a filter object associated with tools\nthat require approval.\n').or(zod.enum(['always', 'never']).describe('Specify a single approval policy for all tools. One of `always` or\n`never`. When set to `always`, all tools will require approval. When\nset to `never`, all tools will not require approval.\n')).optional().describe('Specify which of the MCP server\'s tools require approval.').or(zod.null()).optional()
}).describe('Give the model access to additional tools via remote Model Context Protocol\n(MCP) servers. [Learn more about MCP](https://platform.openai.com/docs/guides/tools-remote-mcp).\n'))).optional().describe('Tools available to the model.'),
  "tool_choice": zod.enum(['none', 'auto', 'required']).describe('Controls which (if any) tool is called by the model.\n\n`none` means the model will not call any tool and instead generates a message.\n\n`auto` means the model can pick between generating a message or calling one or\nmore tools.\n\n`required` means the model must call one or more tools.\n').or(zod.object({
  "type": zod.enum(['function']).describe('For function calling, the type is always `function`.'),
  "name": zod.string().describe('The name of the function to call.')
}).describe('Use this option to force the model to call a specific function.\n')).or(zod.object({
  "type": zod.enum(['mcp']).describe('For MCP tools, the type is always `mcp`.'),
  "server_label": zod.string().describe('The label of the MCP server to use.\n'),
  "name": zod.string().describe('The name of the tool to call on the server.\n').or(zod.null()).optional()
}).describe('Use this option to force the model to call a specific tool on a remote MCP server.\n')).optional().describe('How the model chooses tools. Provide one of the string modes or force a specific\nfunction/MCP tool.\n'),
  "max_output_tokens": zod.number().or(zod.enum(['inf'])).optional().describe('Maximum number of output tokens for a single assistant response,\ninclusive of tool calls. Provide an integer between 1 and 4096 to\nlimit output tokens, or `inf` for the maximum available tokens for a\ngiven model. Defaults to `inf`.\n'),
  "truncation": zod.enum(['auto', 'disabled']).describe('The truncation strategy to use for the session. `auto` is the default truncation strategy. `disabled` will disable truncation and emit errors when the conversation exceeds the input token limit.').or(zod.object({
  "type": zod.enum(['retention_ratio']).describe('Use retention ratio truncation.'),
  "retention_ratio": zod.number().min(createRealtimeClientSecretResponseSessionTruncationRetentionRatioMin).max(createRealtimeClientSecretResponseSessionTruncationRetentionRatioMax).describe('Fraction of post-instruction conversation tokens to retain (0.0 - 1.0) when the conversation exceeds the input token limit.\n')
}).describe('Retain a fraction of the conversation tokens when the conversation exceeds the input token limit. This allows you to amortize truncations across multiple turns, which can help improve cached token usage.')).optional().describe('Controls how the realtime conversation is truncated prior to model inference.\nThe default is `auto`.\n'),
  "prompt": zod.object({
  "id": zod.string().describe('The unique identifier of the prompt template to use.'),
  "version": zod.string().describe('Optional version of the prompt template.').or(zod.null()).optional(),
  "variables": zod.record(zod.string(), zod.string().or(zod.object({
  "type": zod.enum(['input_text']).optional().describe('The type of the input item. Always `input_text`.'),
  "text": zod.string().describe('The text input to the model.')
}).describe('A text input to the model.')).or(zod.object({
  "type": zod.enum(['input_image']).optional().describe('The type of the input item. Always `input_image`.'),
  "image_url": zod.string().describe('The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL.').or(zod.null()).optional(),
  "file_id": zod.string().describe('The ID of the file to be sent to the model.').or(zod.null()).optional(),
  "detail": zod.enum(['low', 'high', 'auto'])
}).describe('An image input to the model. Learn about [image inputs](https://platform.openai.com/docs/guides/vision).')).or(zod.object({
  "type": zod.enum(['input_file']).optional().describe('The type of the input item. Always `input_file`.'),
  "file_id": zod.string().describe('The ID of the file to be sent to the model.').or(zod.null()).optional(),
  "filename": zod.string().optional().describe('The name of the file to be sent to the model.'),
  "file_url": zod.string().optional().describe('The URL of the file to be sent to the model.'),
  "file_data": zod.string().optional().describe('The content of the file to be sent to the model.\n')
}).describe('A file input to the model.'))).describe('Optional map of values to substitute in for variables in your\nprompt. The substitution values can either be strings, or other\nResponse input types like images or files.\n').or(zod.null()).optional()
}).describe('Reference to a prompt template and its variables.\n[Learn more](https://platform.openai.com/docs/guides/text?api-mode=responses#reusable-prompts).\n').or(zod.null()).optional()
}).describe('A new Realtime session configuration, with an ephemeral key. Default TTL\nfor keys is one minute.\n'),zod.object({
  "type": zod.enum(['transcription']).describe('The type of session. Always `transcription` for transcription sessions.\n'),
  "id": zod.string().describe('Unique identifier for the session that looks like `sess_1234567890abcdef`.\n'),
  "object": zod.string().describe('The object type. Always `realtime.transcription_session`.'),
  "expires_at": zod.number().optional().describe('Expiration timestamp for the session, in seconds since epoch.'),
  "include": zod.array(zod.enum(['item.input_audio_transcription.logprobs'])).optional().describe('Additional fields to include in server outputs.\n- `item.input_audio_transcription.logprobs`: Include logprobs for input audio transcription.\n'),
  "audio": zod.object({
  "input": zod.object({
  "format": zod.discriminatedUnion('type', [zod.object({
  "type": zod.enum(['audio/pcm']).optional().describe('The audio format. Always `audio/pcm`.'),
  "rate": zod.number().optional().describe('The sample rate of the audio. Always `24000`.')
}).describe('The PCM audio format. Only a 24kHz sample rate is supported.'),zod.object({
  "type": zod.enum(['audio/pcmu']).optional().describe('The audio format. Always `audio/pcmu`.')
}).describe('The G.711 Œº-law format.'),zod.object({
  "type": zod.enum(['audio/pcma']).optional().describe('The audio format. Always `audio/pcma`.')
}).describe('The G.711 A-law format.')]).optional(),
  "transcription": zod.object({
  "model": zod.enum(['whisper-1', 'gpt-4o-mini-transcribe', 'gpt-4o-transcribe', 'gpt-4o-transcribe-diarize']).optional().describe('The model to use for transcription. Current options are `whisper-1`, `gpt-4o-mini-transcribe`, `gpt-4o-transcribe`, and `gpt-4o-transcribe-diarize`. Use `gpt-4o-transcribe-diarize` when you need diarization with speaker labels.\n'),
  "language": zod.string().optional().describe('The language of the input audio. Supplying the input language in\n[ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`) format\nwill improve accuracy and latency.\n'),
  "prompt": zod.string().optional().describe('An optional text to guide the model\'s style or continue a previous audio\nsegment.\nFor `whisper-1`, the [prompt is a list of keywords](https://platform.openai.com/docs/guides/speech-to-text#prompting).\nFor `gpt-4o-transcribe` models (excluding `gpt-4o-transcribe-diarize`), the prompt is a free text string, for example \"expect words related to technology\".\n')
}).optional(),
  "noise_reduction": zod.object({
  "type": zod.enum(['near_field', 'far_field']).optional().describe('Type of noise reduction. `near_field` is for close-talking microphones such as headphones, `far_field` is for far-field microphones such as laptop or conference room microphones.\n')
}).optional().describe('Configuration for input audio noise reduction.\n'),
  "turn_detection": zod.object({
  "type": zod.string().optional().describe('Type of turn detection, only `server_vad` is currently supported.\n'),
  "threshold": zod.number().optional().describe('Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A\nhigher threshold will require louder audio to activate the model, and\nthus might perform better in noisy environments.\n'),
  "prefix_padding_ms": zod.number().optional().describe('Amount of audio to include before the VAD detected speech (in\nmilliseconds). Defaults to 300ms.\n'),
  "silence_duration_ms": zod.number().optional().describe('Duration of silence to detect speech stop (in milliseconds). Defaults\nto 500ms. With shorter values the model will respond more quickly,\nbut may jump in on short pauses from the user.\n')
}).optional().describe('Configuration for turn detection. Can be set to `null` to turn off. Server\nVAD means that the model will detect the start and end of speech based on\naudio volume and respond at the end of user speech.\n')
}).optional()
}).optional().describe('Configuration for input audio for the session.\n')
}).describe('A Realtime transcription session configuration object.\n')]).describe('The session configuration for either a realtime or transcription session.\n')
}).describe('Response from creating a session and client secret for the Realtime API.\n')

/**
 * Create an ephemeral API token for use in client-side applications with the
Realtime API. Can be configured with the same session parameters as the
`session.update` client event.

It responds with a session object, plus a `client_secret` key which contains
a usable ephemeral API token that can be used to authenticate browser clients
for the Realtime API.

 * @summary Create session
 */
export const createRealtimeSessionBodySpeedDefault = 1;
export const createRealtimeSessionBodySpeedMin = 0.25;

export const createRealtimeSessionBodySpeedMax = 1.5;
export const createRealtimeSessionBodyTracingDefaultOne = "auto";export const createRealtimeSessionBodyTruncationRetentionRatioMin = 0;

export const createRealtimeSessionBodyTruncationRetentionRatioMax = 1;
export const createRealtimeSessionBodyPromptVariablesTypeDefault = "input_text";export const createRealtimeSessionBodyPromptVariablesTypeDefaultOne = "input_image";export const createRealtimeSessionBodyPromptVariablesTypeDefaultTwo = "input_file";

export const createRealtimeSessionBody = zod.object({
  "client_secret": zod.object({
  "value": zod.string().describe('Ephemeral key usable in client environments to authenticate connections\nto the Realtime API. Use this in client-side environments rather than\na standard API token, which should only be used server-side.\n'),
  "expires_at": zod.number().describe('Timestamp for when the token expires. Currently, all tokens expire\nafter one minute.\n')
}).describe('Ephemeral key returned by the API.'),
  "modalities": zod.any().optional().describe('The set of modalities the model can respond with. To disable audio,\nset this to [\"text\"].\n'),
  "instructions": zod.string().optional().describe('The default system instructions (i.e. system message) prepended to model calls. This field allows the client to guide the model on desired responses. The model can be instructed on response content and format, (e.g. \"be extremely succinct\", \"act friendly\", \"here are examples of good responses\") and on audio behavior (e.g. \"talk quickly\", \"inject emotion into your voice\", \"laugh frequently\"). The instructions are not guaranteed to be followed by the model, but they provide guidance to the model on the desired behavior.\nNote that the server sets default instructions which will be used if this field is not set and are visible in the `session.created` event at the start of the session.\n'),
  "voice": zod.string().or(zod.enum(['alloy', 'ash', 'ballad', 'coral', 'echo', 'sage', 'shimmer', 'verse', 'marin', 'cedar'])).optional(),
  "input_audio_format": zod.string().optional().describe('The format of input audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.\n'),
  "output_audio_format": zod.string().optional().describe('The format of output audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.\n'),
  "input_audio_transcription": zod.object({
  "model": zod.string().optional().describe('The model to use for transcription.\n')
}).optional().describe('Configuration for input audio transcription, defaults to off and can be\nset to `null` to turn off once on. Input audio transcription is not native\nto the model, since the model consumes audio directly. Transcription runs\nasynchronously and should be treated as rough guidance\nrather than the representation understood by the model.\n'),
  "speed": zod.number().min(createRealtimeSessionBodySpeedMin).max(createRealtimeSessionBodySpeedMax).optional().describe('The speed of the model\'s spoken response. 1.0 is the default speed. 0.25 is\nthe minimum speed. 1.5 is the maximum speed. This value can only be changed\nin between model turns, not while a response is in progress.\n'),
  "tracing": zod.enum(['auto']).optional().describe('Default tracing mode for the session.\n').or(zod.object({
  "workflow_name": zod.string().optional().describe('The name of the workflow to attach to this trace. This is used to\nname the trace in the traces dashboard.\n'),
  "group_id": zod.string().optional().describe('The group id to attach to this trace to enable filtering and\ngrouping in the traces dashboard.\n'),
  "metadata": zod.object({

}).optional().describe('The arbitrary metadata to attach to this trace to enable\nfiltering in the traces dashboard.\n')
}).describe('Granular configuration for tracing.\n')).optional().describe('Configuration options for tracing. Set to null to disable tracing. Once\ntracing is enabled for a session, the configuration cannot be modified.\n\n`auto` will create a trace for the session with default values for the\nworkflow name, group id, and metadata.\n'),
  "turn_detection": zod.object({
  "type": zod.string().optional().describe('Type of turn detection, only `server_vad` is currently supported.\n'),
  "threshold": zod.number().optional().describe('Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A\nhigher threshold will require louder audio to activate the model, and\nthus might perform better in noisy environments.\n'),
  "prefix_padding_ms": zod.number().optional().describe('Amount of audio to include before the VAD detected speech (in\nmilliseconds). Defaults to 300ms.\n'),
  "silence_duration_ms": zod.number().optional().describe('Duration of silence to detect speech stop (in milliseconds). Defaults\nto 500ms. With shorter values the model will respond more quickly,\nbut may jump in on short pauses from the user.\n')
}).optional().describe('Configuration for turn detection. Can be set to `null` to turn off. Server\nVAD means that the model will detect the start and end of speech based on\naudio volume and respond at the end of user speech.\n'),
  "tools": zod.array(zod.object({
  "type": zod.enum(['function']).optional().describe('The type of the tool, i.e. `function`.'),
  "name": zod.string().optional().describe('The name of the function.'),
  "description": zod.string().optional().describe('The description of the function, including guidance on when and how\nto call it, and guidance about what to tell the user when calling\n(if anything).\n'),
  "parameters": zod.object({

}).optional().describe('Parameters of the function in JSON Schema.')
})).optional().describe('Tools (functions) available to the model.'),
  "tool_choice": zod.string().optional().describe('How the model chooses tools. Options are `auto`, `none`, `required`, or\nspecify a function.\n'),
  "temperature": zod.number().optional().describe('Sampling temperature for the model, limited to [0.6, 1.2]. Defaults to 0.8.\n'),
  "max_response_output_tokens": zod.number().or(zod.enum(['inf'])).optional().describe('Maximum number of output tokens for a single assistant response,\ninclusive of tool calls. Provide an integer between 1 and 4096 to\nlimit output tokens, or `inf` for the maximum available tokens for a\ngiven model. Defaults to `inf`.\n'),
  "truncation": zod.enum(['auto', 'disabled']).describe('The truncation strategy to use for the session. `auto` is the default truncation strategy. `disabled` will disable truncation and emit errors when the conversation exceeds the input token limit.').or(zod.object({
  "type": zod.enum(['retention_ratio']).describe('Use retention ratio truncation.'),
  "retention_ratio": zod.number().min(createRealtimeSessionBodyTruncationRetentionRatioMin).max(createRealtimeSessionBodyTruncationRetentionRatioMax).describe('Fraction of post-instruction conversation tokens to retain (0.0 - 1.0) when the conversation exceeds the input token limit.\n')
}).describe('Retain a fraction of the conversation tokens when the conversation exceeds the input token limit. This allows you to amortize truncations across multiple turns, which can help improve cached token usage.')).optional().describe('Controls how the realtime conversation is truncated prior to model inference.\nThe default is `auto`.\n'),
  "prompt": zod.object({
  "id": zod.string().describe('The unique identifier of the prompt template to use.'),
  "version": zod.string().describe('Optional version of the prompt template.').or(zod.null()).optional(),
  "variables": zod.record(zod.string(), zod.string().or(zod.object({
  "type": zod.enum(['input_text']).optional().describe('The type of the input item. Always `input_text`.'),
  "text": zod.string().describe('The text input to the model.')
}).describe('A text input to the model.')).or(zod.object({
  "type": zod.enum(['input_image']).optional().describe('The type of the input item. Always `input_image`.'),
  "image_url": zod.string().describe('The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL.').or(zod.null()).optional(),
  "file_id": zod.string().describe('The ID of the file to be sent to the model.').or(zod.null()).optional(),
  "detail": zod.enum(['low', 'high', 'auto'])
}).describe('An image input to the model. Learn about [image inputs](https://platform.openai.com/docs/guides/vision).')).or(zod.object({
  "type": zod.enum(['input_file']).optional().describe('The type of the input item. Always `input_file`.'),
  "file_id": zod.string().describe('The ID of the file to be sent to the model.').or(zod.null()).optional(),
  "filename": zod.string().optional().describe('The name of the file to be sent to the model.'),
  "file_url": zod.string().optional().describe('The URL of the file to be sent to the model.'),
  "file_data": zod.string().optional().describe('The content of the file to be sent to the model.\n')
}).describe('A file input to the model.'))).describe('Optional map of values to substitute in for variables in your\nprompt. The substitution values can either be strings, or other\nResponse input types like images or files.\n').or(zod.null()).optional()
}).describe('Reference to a prompt template and its variables.\n[Learn more](https://platform.openai.com/docs/guides/text?api-mode=responses#reusable-prompts).\n').or(zod.null()).optional()
}).describe('A new Realtime session configuration, with an ephemeral key. Default TTL\nfor keys is one minute.\n')

export const createRealtimeSessionResponseTracingDefaultOne = "auto";

export const createRealtimeSessionResponse = zod.object({
  "id": zod.string().optional().describe('Unique identifier for the session that looks like `sess_1234567890abcdef`.\n'),
  "object": zod.string().optional().describe('The object type. Always `realtime.session`.'),
  "expires_at": zod.number().optional().describe('Expiration timestamp for the session, in seconds since epoch.'),
  "include": zod.array(zod.enum(['item.input_audio_transcription.logprobs'])).optional().describe('Additional fields to include in server outputs.\n- `item.input_audio_transcription.logprobs`: Include logprobs for input audio transcription.\n'),
  "model": zod.string().optional().describe('The Realtime model used for this session.'),
  "output_modalities": zod.any().optional().describe('The set of modalities the model can respond with. To disable audio,\nset this to [\"text\"].\n'),
  "instructions": zod.string().optional().describe('The default system instructions (i.e. system message) prepended to model\ncalls. This field allows the client to guide the model on desired\nresponses. The model can be instructed on response content and format,\n(e.g. \"be extremely succinct\", \"act friendly\", \"here are examples of good\nresponses\") and on audio behavior (e.g. \"talk quickly\", \"inject emotion\ninto your voice\", \"laugh frequently\"). The instructions are not guaranteed\nto be followed by the model, but they provide guidance to the model on the\ndesired behavior.\n\nNote that the server sets default instructions which will be used if this\nfield is not set and are visible in the `session.created` event at the\nstart of the session.\n'),
  "audio": zod.object({
  "input": zod.object({
  "format": zod.discriminatedUnion('type', [zod.object({
  "type": zod.enum(['audio/pcm']).optional().describe('The audio format. Always `audio/pcm`.'),
  "rate": zod.number().optional().describe('The sample rate of the audio. Always `24000`.')
}).describe('The PCM audio format. Only a 24kHz sample rate is supported.'),zod.object({
  "type": zod.enum(['audio/pcmu']).optional().describe('The audio format. Always `audio/pcmu`.')
}).describe('The G.711 Œº-law format.'),zod.object({
  "type": zod.enum(['audio/pcma']).optional().describe('The audio format. Always `audio/pcma`.')
}).describe('The G.711 A-law format.')]).optional(),
  "transcription": zod.object({
  "model": zod.enum(['whisper-1', 'gpt-4o-mini-transcribe', 'gpt-4o-transcribe', 'gpt-4o-transcribe-diarize']).optional().describe('The model to use for transcription. Current options are `whisper-1`, `gpt-4o-mini-transcribe`, `gpt-4o-transcribe`, and `gpt-4o-transcribe-diarize`. Use `gpt-4o-transcribe-diarize` when you need diarization with speaker labels.\n'),
  "language": zod.string().optional().describe('The language of the input audio. Supplying the input language in\n[ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`) format\nwill improve accuracy and latency.\n'),
  "prompt": zod.string().optional().describe('An optional text to guide the model\'s style or continue a previous audio\nsegment.\nFor `whisper-1`, the [prompt is a list of keywords](https://platform.openai.com/docs/guides/speech-to-text#prompting).\nFor `gpt-4o-transcribe` models (excluding `gpt-4o-transcribe-diarize`), the prompt is a free text string, for example \"expect words related to technology\".\n')
}).optional(),
  "noise_reduction": zod.object({
  "type": zod.enum(['near_field', 'far_field']).optional().describe('Type of noise reduction. `near_field` is for close-talking microphones such as headphones, `far_field` is for far-field microphones such as laptop or conference room microphones.\n')
}).optional().describe('Configuration for input audio noise reduction.\n'),
  "turn_detection": zod.object({
  "type": zod.string().optional().describe('Type of turn detection, only `server_vad` is currently supported.\n'),
  "threshold": zod.number().optional(),
  "prefix_padding_ms": zod.number().optional(),
  "silence_duration_ms": zod.number().optional()
}).optional().describe('Configuration for turn detection.\n')
}).optional(),
  "output": zod.object({
  "format": zod.discriminatedUnion('type', [zod.object({
  "type": zod.enum(['audio/pcm']).optional().describe('The audio format. Always `audio/pcm`.'),
  "rate": zod.number().optional().describe('The sample rate of the audio. Always `24000`.')
}).describe('The PCM audio format. Only a 24kHz sample rate is supported.'),zod.object({
  "type": zod.enum(['audio/pcmu']).optional().describe('The audio format. Always `audio/pcmu`.')
}).describe('The G.711 Œº-law format.'),zod.object({
  "type": zod.enum(['audio/pcma']).optional().describe('The audio format. Always `audio/pcma`.')
}).describe('The G.711 A-law format.')]).optional(),
  "voice": zod.string().or(zod.enum(['alloy', 'ash', 'ballad', 'coral', 'echo', 'sage', 'shimmer', 'verse', 'marin', 'cedar'])).optional(),
  "speed": zod.number().optional()
}).optional()
}).optional().describe('Configuration for input and output audio for the session.\n'),
  "tracing": zod.enum(['auto']).optional().describe('Default tracing mode for the session.\n').or(zod.object({
  "workflow_name": zod.string().optional().describe('The name of the workflow to attach to this trace. This is used to\nname the trace in the traces dashboard.\n'),
  "group_id": zod.string().optional().describe('The group id to attach to this trace to enable filtering and\ngrouping in the traces dashboard.\n'),
  "metadata": zod.object({

}).optional().describe('The arbitrary metadata to attach to this trace to enable\nfiltering in the traces dashboard.\n')
}).describe('Granular configuration for tracing.\n')).optional().describe('Configuration options for tracing. Set to null to disable tracing. Once\ntracing is enabled for a session, the configuration cannot be modified.\n\n`auto` will create a trace for the session with default values for the\nworkflow name, group id, and metadata.\n'),
  "turn_detection": zod.object({
  "type": zod.string().optional().describe('Type of turn detection, only `server_vad` is currently supported.\n'),
  "threshold": zod.number().optional().describe('Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A\nhigher threshold will require louder audio to activate the model, and\nthus might perform better in noisy environments.\n'),
  "prefix_padding_ms": zod.number().optional().describe('Amount of audio to include before the VAD detected speech (in\nmilliseconds). Defaults to 300ms.\n'),
  "silence_duration_ms": zod.number().optional().describe('Duration of silence to detect speech stop (in milliseconds). Defaults\nto 500ms. With shorter values the model will respond more quickly,\nbut may jump in on short pauses from the user.\n')
}).optional().describe('Configuration for turn detection. Can be set to `null` to turn off. Server\nVAD means that the model will detect the start and end of speech based on\naudio volume and respond at the end of user speech.\n'),
  "tools": zod.array(zod.object({
  "type": zod.enum(['function']).optional().describe('The type of the tool, i.e. `function`.'),
  "name": zod.string().optional().describe('The name of the function.'),
  "description": zod.string().optional().describe('The description of the function, including guidance on when and how\nto call it, and guidance about what to tell the user when calling\n(if anything).\n'),
  "parameters": zod.object({

}).optional().describe('Parameters of the function in JSON Schema.')
})).optional().describe('Tools (functions) available to the model.'),
  "tool_choice": zod.string().optional().describe('How the model chooses tools. Options are `auto`, `none`, `required`, or\nspecify a function.\n'),
  "max_output_tokens": zod.number().or(zod.enum(['inf'])).optional().describe('Maximum number of output tokens for a single assistant response,\ninclusive of tool calls. Provide an integer between 1 and 4096 to\nlimit output tokens, or `inf` for the maximum available tokens for a\ngiven model. Defaults to `inf`.\n')
}).describe('A Realtime session configuration object.\n')

/**
 * Create an ephemeral API token for use in client-side applications with the
Realtime API specifically for realtime transcriptions.
Can be configured with the same session parameters as the `transcription_session.update` client event.

It responds with a session object, plus a `client_secret` key which contains
a usable ephemeral API token that can be used to authenticate browser clients
for the Realtime API.

 * @summary Create transcription session
 */
export const createRealtimeTranscriptionSessionBodyInputAudioFormatDefault = "pcm16";

export const createRealtimeTranscriptionSessionBody = zod.object({
  "turn_detection": zod.object({
  "type": zod.enum(['server_vad']).optional().describe('Type of turn detection. Only `server_vad` is currently supported for transcription sessions.\n'),
  "threshold": zod.number().optional().describe('Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A\nhigher threshold will require louder audio to activate the model, and\nthus might perform better in noisy environments.\n'),
  "prefix_padding_ms": zod.number().optional().describe('Amount of audio to include before the VAD detected speech (in\nmilliseconds). Defaults to 300ms.\n'),
  "silence_duration_ms": zod.number().optional().describe('Duration of silence to detect speech stop (in milliseconds). Defaults\nto 500ms. With shorter values the model will respond more quickly,\nbut may jump in on short pauses from the user.\n')
}).optional().describe('Configuration for turn detection. Can be set to `null` to turn off. Server VAD means that the model will detect the start and end of speech based on audio volume and respond at the end of user speech.\n'),
  "input_audio_noise_reduction": zod.object({
  "type": zod.enum(['near_field', 'far_field']).optional().describe('Type of noise reduction. `near_field` is for close-talking microphones such as headphones, `far_field` is for far-field microphones such as laptop or conference room microphones.\n')
}).optional().describe('Configuration for input audio noise reduction. This can be set to `null` to turn off.\nNoise reduction filters audio added to the input audio buffer before it is sent to VAD and the model.\nFiltering the audio can improve VAD and turn detection accuracy (reducing false positives) and model performance by improving perception of the input audio.\n'),
  "input_audio_format": zod.enum(['pcm16', 'g711_ulaw', 'g711_alaw']).optional().describe('The format of input audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.\nFor `pcm16`, input audio must be 16-bit PCM at a 24kHz sample rate,\nsingle channel (mono), and little-endian byte order.\n'),
  "input_audio_transcription": zod.object({
  "model": zod.enum(['whisper-1', 'gpt-4o-mini-transcribe', 'gpt-4o-transcribe', 'gpt-4o-transcribe-diarize']).optional().describe('The model to use for transcription. Current options are `whisper-1`, `gpt-4o-mini-transcribe`, `gpt-4o-transcribe`, and `gpt-4o-transcribe-diarize`. Use `gpt-4o-transcribe-diarize` when you need diarization with speaker labels.\n'),
  "language": zod.string().optional().describe('The language of the input audio. Supplying the input language in\n[ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`) format\nwill improve accuracy and latency.\n'),
  "prompt": zod.string().optional().describe('An optional text to guide the model\'s style or continue a previous audio\nsegment.\nFor `whisper-1`, the [prompt is a list of keywords](https://platform.openai.com/docs/guides/speech-to-text#prompting).\nFor `gpt-4o-transcribe` models (excluding `gpt-4o-transcribe-diarize`), the prompt is a free text string, for example \"expect words related to technology\".\n')
}).optional(),
  "include": zod.array(zod.enum(['item.input_audio_transcription.logprobs'])).optional().describe('The set of items to include in the transcription. Current available items are:\n`item.input_audio_transcription.logprobs`\n')
}).describe('Realtime transcription session object configuration.')

export const createRealtimeTranscriptionSessionResponse = zod.object({
  "client_secret": zod.object({
  "value": zod.string().describe('Ephemeral key usable in client environments to authenticate connections\nto the Realtime API. Use this in client-side environments rather than\na standard API token, which should only be used server-side.\n'),
  "expires_at": zod.number().describe('Timestamp for when the token expires. Currently, all tokens expire\nafter one minute.\n')
}).describe('Ephemeral key returned by the API. Only present when the session is\ncreated on the server via REST API.\n'),
  "modalities": zod.any().optional().describe('The set of modalities the model can respond with. To disable audio,\nset this to [\"text\"].\n'),
  "input_audio_format": zod.string().optional().describe('The format of input audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.\n'),
  "input_audio_transcription": zod.object({
  "model": zod.enum(['whisper-1', 'gpt-4o-mini-transcribe', 'gpt-4o-transcribe', 'gpt-4o-transcribe-diarize']).optional().describe('The model to use for transcription. Current options are `whisper-1`, `gpt-4o-mini-transcribe`, `gpt-4o-transcribe`, and `gpt-4o-transcribe-diarize`. Use `gpt-4o-transcribe-diarize` when you need diarization with speaker labels.\n'),
  "language": zod.string().optional().describe('The language of the input audio. Supplying the input language in\n[ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`) format\nwill improve accuracy and latency.\n'),
  "prompt": zod.string().optional().describe('An optional text to guide the model\'s style or continue a previous audio\nsegment.\nFor `whisper-1`, the [prompt is a list of keywords](https://platform.openai.com/docs/guides/speech-to-text#prompting).\nFor `gpt-4o-transcribe` models (excluding `gpt-4o-transcribe-diarize`), the prompt is a free text string, for example \"expect words related to technology\".\n')
}).optional(),
  "turn_detection": zod.object({
  "type": zod.string().optional().describe('Type of turn detection, only `server_vad` is currently supported.\n'),
  "threshold": zod.number().optional().describe('Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A\nhigher threshold will require louder audio to activate the model, and\nthus might perform better in noisy environments.\n'),
  "prefix_padding_ms": zod.number().optional().describe('Amount of audio to include before the VAD detected speech (in\nmilliseconds). Defaults to 300ms.\n'),
  "silence_duration_ms": zod.number().optional().describe('Duration of silence to detect speech stop (in milliseconds). Defaults\nto 500ms. With shorter values the model will respond more quickly,\nbut may jump in on short pauses from the user.\n')
}).optional().describe('Configuration for turn detection. Can be set to `null` to turn off. Server\nVAD means that the model will detect the start and end of speech based on\naudio volume and respond at the end of user speech.\n')
}).describe('A new Realtime transcription session configuration.\n\nWhen a session is created on the server via REST API, the session object\nalso contains an ephemeral key. Default TTL for keys is 10 minutes. This\nproperty is not present when a session is updated via the WebSocket API.\n')

